{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerias para el transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForQuestionAnswering: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer, TFRobertaForQuestionAnswering\n",
    "\n",
    "# Cargar el modelo y el tokenizador preentrenados\n",
    "model_name = 'deepset/roberta-large-squad2'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = TFRobertaForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contexto de ejemplo\n",
    "contexto = \"\"\"\n",
    "Los Transformers, introducidos por Google en 2017, son una arquitectura de red neuronal que revolucionó el procesamiento del lenguaje natural (PLN) gracias a su mecanismo de atención.\n",
    "A diferencia de las redes recurrentes (RNN), que procesan secuencias palabra por palabra, los Transformers comprenden las relaciones de largo alcance entre las palabras usando atención, permitiéndoles capturar mejor el contexto y ser más precisos en tareas como traducción automática, resumen de texto y respuesta a preguntas. Esta arquitectura también se ha aplicado con éxito en otras áreas como la visión artificial y el procesamiento del habla.\n",
    "\"\"\"\n",
    "\n",
    "# Pregunta de ejemplo\n",
    "pregunta = \"¿Como comprenden las relaciones?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar las entradas\n",
    "inputs = tokenizer(pregunta, contexto, add_special_tokens=True, return_tensors=\"tf\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Obtener las posiciones de las respuestas\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "respuesta_inicios = tf.math.argmax(outputs.start_logits, axis=-1).numpy()[0]\n",
    "respuesta_fines = tf.math.argmax(outputs.end_logits, axis=-1).numpy()[0]\n",
    "\n",
    "# Recuperar los tokens correspondientes a la respuesta\n",
    "respuesta_tokens = input_ids[0, respuesta_inicios : respuesta_fines + 1]\n",
    "respuesta = tokenizer.decode(respuesta_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¿Como comprenden las relaciones?\n",
      "Respuesta:  de largo alcance entre las palabras usando atención\n",
      "Pregunta: ¿Como comprenden las relaciones?\n",
      "Respuesta:  de largo alcance entre las palabras usando atención\n"
     ]
    }
   ],
   "source": [
    "# Imprimir la respuesta# Imprimir la respuesta\n",
    "print(f\"Pregunta: {pregunta}\")\n",
    "print(f\"Respuesta: {respuesta}\")\n",
    "print(f\"Pregunta: {pregunta}\")\n",
    "print(f\"Respuesta: {respuesta}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
