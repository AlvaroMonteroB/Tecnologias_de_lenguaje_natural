{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b9ba9d-4b55-4e19-9383-534c73effd33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Cabecera**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1360cba-cea4-43a1-bb56-6305bf7d68d8",
   "metadata": {},
   "source": [
    "Gutierrez Viveros Cristian Rogelio \n",
    "\n",
    "5BV1 \n",
    "\n",
    "Ingenieria en Inteligencia Artificial \n",
    "\n",
    "Ultima modificacion: 12/01/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a093ed58-f698-4297-ac4f-b44856ab2417",
   "metadata": {},
   "source": [
    "Práctica 5. Analisis de sentimientos\n",
    "\n",
    "A lo largo de esta practica, utilizaremos diversos modelos para el analisis de sentimientos, utilizando modelos de machine learning, diccionarios y redes neuronales. Ademas de que realizaremos un analisis de una coleccion de reseñas de comida fina en amazon y finalmente compararemos los distintos modelos al ponerlos a prueba con el dataset normalizado y vectorizado segun el modelo que corresponda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace49ab7-ce5e-4c39-9e5e-48099807ab98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Modulos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e52268f-f173-4e99-88f2-7373617bfe8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\Crist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Crist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Crist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pysentiment2 as ps\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import nltk\n",
    "nltk.download('opinion_lexicon')\n",
    "from nltk.corpus import opinion_lexicon\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f8cfad7-599d-49f7-a1e5-3f08134a6b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Crist\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56523bf-1b1c-49a6-94e0-e462a382b1a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Funciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87308371-a91a-444a-84b5-6921475e08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el CSV\n",
    "def leer_datos(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf-8') as archivo: \n",
    "        reader = csv.reader(archivo)\n",
    "        header = next(reader, None)  # Obtener la primera línea como encabezado\n",
    "        for linea in reader:\n",
    "            data.append(linea)\n",
    "\n",
    "    return header, data\n",
    "\n",
    "def leer_datos_noheader(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf-8') as archivo: \n",
    "        reader = csv.reader(archivo)\n",
    "        for linea in reader:\n",
    "            data.append(linea)\n",
    "    return data\n",
    "\n",
    "def randomize(lista):\n",
    "    randomized = lista[:]  \n",
    "    random.shuffle(randomized)\n",
    "    return randomized\n",
    "\n",
    "# Elimina etiquetas HTML y comentarios dentro del propio texto como indicaciones extra entre parentesis o comentarios entre --\n",
    "def eliminar_etiquetas(text):\n",
    "    pattern = re.compile(r'<[^>]+>|\\([^)]+\\)|--[^\\-]+--')\n",
    "    doc = pattern.sub('', text)\n",
    "    return doc\n",
    "\n",
    "# Removemos apostrofes para quedarnos con palabras como cant couldnt o dont\n",
    "def remover_apostrofes(text):\n",
    "    pattern = r\"\\\\?'\"\n",
    "    doc = re.sub(pattern, '', text)\n",
    "    return doc\n",
    "# Nos quedamos con solo alfabeto eliminando caracteres especiales    \n",
    "def remover_especiales(text):\n",
    "    pattern = r\"[^a-z\\s]\"\n",
    "    doc = re.sub(pattern, ' ', text)\n",
    "    return doc\n",
    "# Quitamos dobles espacios para tener texto espaciado solamente por un espacio por palabra\n",
    "def dobles_espacios(text):\n",
    "    pattern = r'\\s+'\n",
    "    doc = re.sub(pattern, ' ', text)\n",
    "    return doc\n",
    "    \n",
    "# Removemos stop words unicamente en ingles\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "# Lematizacion mediante spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def lemmatize(doc):\n",
    "    docs = list(nlp.pipe([doc]))\n",
    "    lemas = ' '.join([token.lemma_ for doc in docs for token in doc])\n",
    "    return lemas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f28332b-e1ab-4594-9818-e0bea136fa6d",
   "metadata": {},
   "source": [
    "Funciones destinadas para analisis de sentimientos con diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "862ffd33-d76c-4f56-b287-c6164d986cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hiv4_polarity(text,hiv4):\n",
    "    tokens = hiv4.tokenize(text)  \n",
    "    score = hiv4.get_score(tokens)\n",
    "\n",
    "    polarity = score['Polarity']\n",
    "    if polarity > 0.33:\n",
    "        return \"Positivo\"\n",
    "    elif polarity < -0.33:\n",
    "        return \"Negativo\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenizar el texto\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Obtener palabras positivas y negativas del Opinion Lexicon\n",
    "    pos_words = set(opinion_lexicon.positive())\n",
    "    neg_words = set(opinion_lexicon.negative())\n",
    "\n",
    "    # Calcular el sentimiento\n",
    "    pos_score = sum(word in pos_words for word in tokens)\n",
    "    neg_score = sum(word in neg_words for word in tokens)\n",
    "\n",
    "    # Determinar la orientación general\n",
    "    if pos_score > neg_score:\n",
    "        return \"Positivo\"\n",
    "    elif pos_score < neg_score:\n",
    "        return \"Negativo\"\n",
    "    else:\n",
    "        return \"Neutro\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d830c-052d-484a-8cfa-62406ccf3c25",
   "metadata": {},
   "source": [
    "Funciones de vectorizacion de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4afc82b9-ec71-4788-9a67-9ecf3e4b0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizar_tfidf(xd):\n",
    "    # TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(xd)\n",
    "    \n",
    "    # palabras únicas\n",
    "    feature_names_tfidf = vectorizer.get_feature_names_out()\n",
    "\n",
    "    print(\"Tamaño de Palabras únicas:\", len(feature_names_tfidf))\n",
    "    return tfidf_matrix\n",
    "\n",
    "def vectorizar_onehot(xd):\n",
    "    # One-Hot Encoding\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    onehot_matrix = vectorizer.fit_transform(xd)\n",
    "    return onehot_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c643f-facf-4625-9eba-b6a83d99a501",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Análisis exploratorio de datos y preprocesamiento**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6aead-ebfa-417d-9003-b2f2214c81dd",
   "metadata": {},
   "source": [
    "Para la lectura de archivos, leeremos el CSV de las reseñas, tomando la primer linea como la cabecera con los titulos de cada columna, y el resto en data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "911424c2-cec9-40ac-8f0e-d28fb9c7038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getcwd()\n",
    "path = (directory+\"\\Reviews.csv\")\n",
    "header, data = leer_datos(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da85c786-faf2-47ae-b110-8ef1de7b0f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hay 568454 datos. Cada uno con 10 caracteristicas\n",
      "columnas:\n",
      "['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'] \n",
      "\n",
      "['2', 'B00813GRG4', 'A1D87F6ZCVE5NK', 'dll pa', '0', '0', '1', '1346976000', 'Not as Advertised', 'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".'] \n",
      "\n",
      "['3', 'B000LQOCH0', 'ABXLMWJIXXAIN', 'Natalia Corres \"Natalia Corres\"', '1', '1', '4', '1219017600', '\"Delight\" says it all', 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.']\n"
     ]
    }
   ],
   "source": [
    "print(\"hay\",len(data),\"datos. Cada uno con\",len(header),\"caracteristicas\")\n",
    "print(\"columnas:\")\n",
    "print(header,\"\\n\")\n",
    "print(data[1], \"\\n\")\n",
    "print(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47e1e4a7-4617-4914-bde5-a60b054a05a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de la primera fila:\n",
      "Id: 2  (tipo: <class 'str'> )\n",
      "ProductId: B00813GRG4  (tipo: <class 'str'> )\n",
      "UserId: A1D87F6ZCVE5NK  (tipo: <class 'str'> )\n",
      "ProfileName: dll pa  (tipo: <class 'str'> )\n",
      "HelpfulnessNumerator: 0  (tipo: <class 'str'> )\n",
      "HelpfulnessDenominator: 0  (tipo: <class 'str'> )\n",
      "Score: 1  (tipo: <class 'str'> )\n",
      "Time: 1346976000  (tipo: <class 'str'> )\n",
      "Summary: Not as Advertised  (tipo: <class 'str'> )\n",
      "Text: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".  (tipo: <class 'str'> )\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Datos de la primera fila:\")\n",
    "for i in range(len(header)):\n",
    "    print(header[i] + \":\", data[1][i], \" (tipo:\", type(data[1][i]), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13a6efd-d48b-4a7a-8795-da607773fd50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Segmentacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860bf546-2177-4054-9422-f1cee2a5992a",
   "metadata": {},
   "source": [
    "Guardamos el vector de reviews solamente con las columnas del score, el summary y el texto de la reseña"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bdb1d117-329a-4d63-a556-40d9d38e770b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '\"Delight\" says it all', 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.']\n"
     ]
    }
   ],
   "source": [
    "columnas = [6,8,9]\n",
    "reviews = [[row[i] for i in columnas] for row in data]\n",
    "print(reviews[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9ba21-4296-4dfb-b504-425fc0a9bd95",
   "metadata": {},
   "source": [
    "Convertimos el score de las reseñas a Negativo, Neutral y Positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7f4ab8d-ce8f-4399-a1e5-771bc0b02235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82037, 42640, 443777]\n"
     ]
    }
   ],
   "source": [
    "label_counter = [0,0,0]\n",
    "for i in range(len(reviews)):\n",
    "    if reviews[i][0] == '1' or reviews[i][0] == '2':\n",
    "        reviews[i][0] = 'Negativo'\n",
    "        label_counter[0] += 1\n",
    "    elif reviews[i][0] == '3':\n",
    "        reviews[i][0] = 'Neutral'\n",
    "        label_counter[1] += 1\n",
    "    else:\n",
    "        reviews[i][0] = 'Positivo'\n",
    "        label_counter[2] += 1\n",
    "print(label_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aadeca-8efe-4012-846b-77addef47f00",
   "metadata": {},
   "source": [
    "Mezclamos las reseñas con su respectiva etiqueta y nos quedamos solamente con las primeras 40000 reseñas de cada etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd6e3164-de09-40d2-a23b-24444191b2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positivo', 'Good', 'This is a tasty trail mix with a blend of nuts, sesame sticks, and chocolate covered soy pieces. Each packet is a single serving of 1.5 ounces, which is not very much - maybe about 2 tablespoons.  This was a good deal when I bought it for $20.77.  It is not a good deal at $43.77!']\n"
     ]
    }
   ],
   "source": [
    "reviews = randomize(reviews)\n",
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "116943ec-53de-4dfc-9a5d-a7561d6317d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40000, 40000, 40000]\n",
      "120000\n"
     ]
    }
   ],
   "source": [
    "reviews_compressed = []\n",
    "label_check = [0,0,0]\n",
    "for i in range(len(reviews)):\n",
    "    if label_check[0] == label_check[1] == label_check[2] == 40000:\n",
    "        break\n",
    "    if reviews[i][0] == 'Positivo' and label_check[0] < 40000:\n",
    "        reviews_compressed.append(reviews[i])\n",
    "        label_check[0] += 1\n",
    "    elif reviews[i][0] == 'Neutral' and label_check[1] < 40000:\n",
    "        reviews_compressed.append(reviews[i])\n",
    "        label_check[1] += 1\n",
    "    elif reviews[i][0] == 'Negativo' and label_check[2] < 40000:\n",
    "        reviews_compressed.append(reviews[i])\n",
    "        label_check[2] += 1\n",
    "print(label_check)\n",
    "print(len(reviews_compressed))\n",
    "reviews = reviews_compressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b51ad-b538-4227-8f14-d3f6a1e0d47f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Limpieza de Texto**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f567f-b805-46a6-a1e2-f533a126dfe5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab393aac-7bbb-413d-ab47-7d4e12de1870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't waste your money on any of the Kettle brand potato chips.  I bought a case of these, and a case of the cheddar and sour cream.  Both cases ended up in the garbage can. \n",
      "\n",
      "DEfintely not as tasty as the Madhouse Munchies which are my family's favorite. Kettle's are dark/burnt,more broken bits, taste greasy-oily and not the light crunch. Oh well. \n",
      "\n",
      "I love sour food but this one I can't bear.  Too strong sour taste... and even when you open the bag, the sour smell is too strong.  I don't like it. \n",
      "\n",
      "Unless you really really really like vinegar - AVOID! Those chips should have been called \"Vinegar and Sea Salt\" - not \"Sea Salt and Vinegar\". \n",
      "\n",
      "I have loved Kettle Brand Sea Salt and Vinegar chips since the first time i tried them.  The fact that i was able to find them on-line for such a great price, was wonderful.  I would definitely make the purchase again.  THey are a quick and simple snack for lunch and it goes great with my cold sandwich. \n",
      "\n",
      "Used to eat the Spicy Thai flavor all the time.  MSG makes my body unhappy and this was one of the few flavored chips that was MSG free.  Now they have changed the whole recipe and how they make them.  Instead of being real food on the ingredients like it used to be its mostly processed chemical garbage, a bunch more salt and MSG stuffed in under the label \"yeast extract\".  They removed the NO MSG from the label so they know very well what they did.<br /><br />The worst part though is that they taste horrible.  Instead of crisp, oily and full of character...say as if they were cooked in like...a kettle or something they instead have the color and uniformity of baked lays.  The Spicy Thai flavor now tastes like sour cream and onion or ranch that has gone rancid.<br /><br />What a shame, no more kettle chips for me.  It seems they are slowly moving one flavor at a time to this new cheap ingredient list and manufacturing method.  Dont be fooled into paying a premium price for these chips as they are not a premium product anymore. \n",
      "\n",
      "The chips were great...for the first few bags. However, after the first bag or two, I noticed that the remaining bags were damaged. There were holes in each bag and black, sticky stains on the outside. Other reviewers who bought around the same time that I did are now claiming that they found rodent holes in their bags of chips.<br /><br />The chips came in a sealed Kettle box, so it's hard to say who screwed up. But someone somewhere screwed up. These chips were a health hazard before the holes in the bag. Now? It's like a game of roulette. \n",
      "\n",
      "After waiting a ridiculous amount of time for my case of 15 5oz bags to arrive, upon opening the box noticed that every bag had been chewed open by a mouse.  Don't know if it is still in the box, but it is outside on the porch.  When my son gets home, I'm going to have pictures taken and email to Amazon.  The outside Amazon box was intact, so it had to be either from the Kettle Chips people or the Amazon warehouse, don't know which.  I will never buy these again from Amazon.  I cannot tell you how disgusted I am with this purchase.  It makes me sick to think about it!<br /><br />I included pictures at the top of the page.  Poor little mousie must have high cholesterol now.<br /><br />Update:  I forgot to mention that I offered to email the above pictures of the mouse damage to Amazon but was told they didn't want them. \n",
      "\n",
      "Kettle Chips are the best potato chip God has ever invented.  I give the Lord thanks every day for delivering unto me such an incredulously delicious blend of ginger and spice, a veritable cornucopia of flavor.  I have actually changed my diet to a strict regiment of the Spicy Thai & Sea Salt and Vinegar flavors, alternating days. I have already lost 5 lbs, not to mention the myriad of other health benefits I have been experiencing. Get your life back - with Kettle Chips. \n",
      "\n",
      "My daughter that has autism craves hot, spice and pungent foods.  These are her absolute favorite chips!  She calls them her sour chips and wants them in her lunch all the time.  I love the crispy kettle way they're cooked. \n",
      "\n",
      "I am a great fan of potato chips and of Thai food.  I was so happy when Kettle Chips decided to meld two of my great loves together! (What a concept.....)  These chips are spicy enough, without burning a hole in your tongue.  Also, they have a nice hint of sweetness that makes them habit forming (careful!).  Once in a while, this is a treat definitely worth indulging in. \n",
      "\n",
      "Terrible! I cannot believe this, I received this item and EVERY SINGLE BAG WAS OPENED BUT 4!!!! I'm stationed in Afghanistan and this was gonna be a snack for my team while going out on missions. I was so embarrassed when the bags were opened and spilt out all in the box, gross! And the box is filled with grease stains. Idk if it was from the airplane ride all the way here, but the box should have been more insulated and bubble wrap should have been used instead of paper. I'm very unhappy with stale crusty chips out the bag and us soldiers cannot enjoy now. \n",
      "\n",
      "After reading some of the reviews, I got nervous and opened a bag from my recent order expecting the worst! No worries here. All bags are in great shape and expiration dates aren't until June. Chips, at least from the first bag, taste like they are supposed to and all is good in the world! By the way, fifteen bags for under thirty dollars is a ton less expensive than the going price around here at the local grocery store so, yay team! \n",
      "\n",
      "Kettle Branch Potato Chips New York Cheddar:  These are good if you like kettle fried potato chips that are waaaay salty, on the burnt side, and taste rancid, either because the cheese flavoring or the oil it was fried in was already old.  I want to like this brand of chips and try their new and other flavors every now and then.  But, after having tried all sorts of other brands of kettle cooked chips, these just don't hit the spot for me. \n",
      "\n",
      "I am glad I was able to find these on this site. I love this flavor and they are so crunchy. The box is packed inside another box but still some bags end up as crumbs. Most of the bags survived in tact. \n",
      "\n",
      "These chips taste awesome. And unlike most other flavored chips, they actually make sure that plenty of the flavory salty goodness gets on each individual chip. Just don't pass gas near any pretty ladies after consumption. They'll totally know it was you. \n",
      "\n",
      "With Kettle Chips, you really have to be careful.  Some of their flavors are nauseating.  With that said, they DO make fantastic plain chips.  Thick cuts of potato, fried to a dark golden brown.  They are crunchy and lightly salted with sea salt.  I can't recommend these chips enough.  You won't regret it.<br /><br />Some people say they are burnt but they aren't.  From their website: \"Take a quick look and you'll see an immediate difference: Kettle Brand® Potato Chips are a beautiful tawny gold. During cooking, the natural sugars of our select potatoes caramelize, creating chips from light gold to a deep amber.  The results are flavors as deep and rich as the colors, and an artisanal display in every bag\"<br /><br />Some have also said that these chips are oily, which is true.  But they explain that on their website: \"At Kettle Foods we exclusively use expeller pressed high monounsaturated sunflower and/or safflower oil to make Kettle Brand®  products. Both of these oils are naturally free of trans fatty acids and are not hydrogenated in any way. We have taken the extra step of sending our products to independent third party labs to test for the presence of trans fats and results indicate we are \"trans fat free\" meaning none were detected. You will see zero trans fats listed on our packaging.\"  That oiliness is natural and happens when quality oil is being used.<br /><br />You will not find chips with such a natural potato flavor.  I'm also a fan of their unsalted potato chips.  For those of us watching our salt intake, the unsalted chips is the best you can get.  Period. \n",
      "\n",
      "These are among the best chips I have ever eaten!  I first came upon them when I visited a CostPlus World Market store in Opelika, AL, in the Tiger Town Mall, in 2007.  I wasn't a big fan of dijon-flavored anything, but I decided to give these chips a try.  I was hooked.  I probably ended up buying over a dozen bags within just a few months' time.  I searched all over my part of Alabama (the northwestern part of the state) and could not find them.  I later tried to find them at the World Market in Hoover, at the Patton Creek Shopping Center, and they did not carry this flavor.  However, when I learned that they were available on Amazon.com, and also available for PRIME Shipping - I bought them as soon as I read \"Kettle Chips Honey Dijon, 9-Ounce Bags (Pack of 12)!\"<br /><br />You get a pretty big box, filled with 12 bags of these delicious morsels.  They last quite a while, but I will re-order, very, very soon!  I suggest you buy them, in this bulk form, and enjoy them - you won't be disappointed!  They go great with hamburgers and hot dogs - so they're perfect for spring and this coming summer's outdoor activities and cook-outs!<br /><br />Take my word for it, these chips are a hidden gem lost in the world of snack foods!  Buy some today, and fall in love like me! \n",
      "\n",
      "I have never been addicted to anything in my life...until I tasted these chips.  I have tried other brands of the sea salt and vinegar flavor and they are just not the same.  You've got to stick with this blue bag of chips! \n",
      "\n",
      "I'm addicted to salty and tangy flavors, so when I opened my first bag of Sea Salt & Vinegar Kettle Brand chips I knew I had a perfect complement to my vegetable trays of cucumber, carrot, celery and cherry tomatoes. Skip the dip; balance the tangy chips by alternating bites of raw vegetable.<br /><br />As an Oregonian, I'm proud to share these delectable snacks with friends, especially those living outside our state and who haven't experienced gourmet chips. I tell them Kettle Brand does for potato chips what microbrews did for beer.<br /><br />Kettle Brand potato chips are unmistakable--a light gold color, rich flavor and amazing crunch. Kettle Brand chips are also a healthier snacking option than the major chip brands. Kettle Brand chips don't have trans fats, MSG or artificial flavors and colorings. The company also has a line of organic potato chips and all of their products are certified Kosher.<br /><br />I also recommend <a href=\"http://www.amazon.com/gp/product/B000G6MBV4\">Kettle Chips Honey Dijon</a> and <a href=\"http://www.amazon.com/gp/product/B000G6Q4GM\">Kettle Chips Spicy Thai</a>.<br /><br />Annette Solomon, a reporter for the Salem Statesman Journal recently noted that a glass of wine goes nicely with these chips. Solomon wrote, \"...you could be missing out on a wonderful pairing. These chips are spicy, so you would want to select a semi-sweet white wine. Also, a moderate amount of acid will subdue the strong flavors of ginger, lime, garlic and cilantro without over-powering them. Classically, a German-style Riesling fits these parameters perfectly.\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range (20):\n",
    "    print(reviews[i+555][2],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b709b9-dcfa-4681-b8ec-fab97652a638",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tecnicas de limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0dbe82f5-0fa3-4534-9484-c66b87315a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reviews)):\n",
    "    reviews[i][2] = eliminar_etiquetas(reviews[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67f4b61a-ff05-44fc-98ba-860e296aa5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reviews)):\n",
    "    reviews[i][1] = reviews[i][1].lower()\n",
    "    reviews[i][2] = reviews[i][2].lower()\n",
    "    reviews[i][1] = remover_apostrofes(reviews[i][1])\n",
    "    reviews[i][2] = remover_apostrofes(reviews[i][2])\n",
    "    reviews[i][1] = remover_especiales(reviews[i][1])\n",
    "    reviews[i][2] = remover_especiales(reviews[i][2])\n",
    "    reviews[i][1] = dobles_espacios(reviews[i][1])\n",
    "    reviews[i][2] = dobles_espacios(reviews[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "848b746e-9810-41da-adc0-4899df648681",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_fixed = reviews.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7eed9cf0-3f29-4dc9-94c1-e2bb4d601b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloque siendo leido (13 bloques en total)\n",
      "Bloque siendo leido (13 bloques en total)\n",
      "Bloque siendo leido (13 bloques en total)\n",
      "Bloque siendo leido (13 bloques en total)\n",
      "Bloque siendo leido (13 bloques en total)\n",
      "Bloque siendo leido (13 bloques en total)\n",
      "Bloque siendo leido (13 bloques en total)\n",
      "Bloque siendo leido (13 bloques en total)\n",
      "Bloque siendo leido (13 bloques en total)\n",
      "Bloque siendo leido (13 bloques en total)\n",
      "Bloque siendo leido (13 bloques en total)\n",
      "Bloque siendo leido (13 bloques en total)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(reviews)):\n",
    "    if i % 10000 == 0:\n",
    "        print(\"Bloque siendo leido (13 bloques en total)\")\n",
    "    reviews[i][1] = remove_stopwords(reviews[i][1])\n",
    "    reviews[i][2] = remove_stopwords(reviews[i][2])\n",
    "    reviews[i][1] = lemmatize(reviews[i][1])\n",
    "    reviews[i][2] = lemmatize(reviews[i][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624af8d1-79bf-4f15-a250-b10bd48cde04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Guardado de resultados tras limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7d5e968-334c-4240-9b72-ad8b1879c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_save = reviews.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2ff1a1-a1e4-4a20-8a40-e1776a218b9b",
   "metadata": {},
   "source": [
    "Ya habia guardado las reseñas normalizadas con anterioridad, solo que lo corri posteriormente al comentar :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "917102f1-4134-4e95-836b-e885dd32ca6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reviews_save' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m archivo_csv:\n\u001b[0;32m      4\u001b[0m     writing \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(archivo_csv)\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fila \u001b[38;5;129;01min\u001b[39;00m \u001b[43mreviews_save\u001b[49m:\n\u001b[0;32m      7\u001b[0m         writing\u001b[38;5;241m.\u001b[39mwriterow(fila)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSe ha guardado como \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m correctamente.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'reviews_save' is not defined"
     ]
    }
   ],
   "source": [
    "filename = \"Reviews_140k.csv\"\n",
    "\n",
    "with open(filename, mode='w', newline='', encoding='utf-8') as archivo_csv:\n",
    "    writing = csv.writer(archivo_csv)\n",
    "\n",
    "    for fila in reviews_save:\n",
    "        writing.writerow(fila)\n",
    "\n",
    "print(f\"Se ha guardado como {filename} correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f13f0677-d184-4da4-a4a9-a4bb183053a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positivo', 'good', 'tasty trail mix blend nuts sesame stick chocolate cover soy piece packet single serve ounce much maybe tablespoon good deal buy good deal'] \n",
      "\n",
      "['Positivo', 'good affordable', 'turn cut sugar diet purely health maintenance measure blood pressure bmi cholesterol blood sugar continue healthy age height despite inactive something get change want stay healthy lifestyle diet preferable manage use splenda packet per day beverage try sweetener least week i d like support stevia use could handle leafy taste even ice tea already leafy concern may much since many web sight warn aluminum buildup liver contact cargill try find amount splenda human trail involve would tell say consumption limit that s take star review wish splendas maker would forthcoming consumer contact legitimate concern tolerate sugar make gallon beverage add one tablespoon sugar per grams splenda soften metallic taste sweetener'] \n",
      "\n",
      "['Positivo', 'one favorite', 'love flavor like try different coffee flavor rank one favs do not like strong coffee perfect work really well flavor creamer well'] \n",
      "\n",
      "['Positivo', 'simply wonderful', 'first try french restaurant las vegas sooo good make sure bring name home try find simply good flavor year purchase raspberry strawberry great'] \n",
      "\n",
      "['Positivo', 'meyer lemon flavor evoo', 'happy purchase get three oz bottle can not beat price I ve see meyer lemon olive oil product twice price great taste versatile oil love I m think orange flavor one'] \n",
      "\n",
      "['Positivo', 'love pill pocket', 'treat perfect disguise hide daily med little furball plus since cost bag store price low shipping fee bargain thank bunch'] \n",
      "\n",
      "['Positivo', 'great product', 'use nicorette quit smoking use gum quit nicorette regular gum bunch thing can not pronounce name give headache go natural gum sugar gum great natural sure xylitol really help tooth far natural sugar free gum go trick highly recommend'] \n",
      "\n",
      "['Negativo', 'like drinking armpit', 'normally purchase tea tea green white unsweetened tea ounce bottle also drink homebrewed unsweetened tea day long please do not write someone hate unsweetene buy bottle tea prefer brand available side city available locally well ill try anything will not hesitate tell world disgusting tea I ve ever misfortune consume tastes foul grant I m still drink darn bottle pay two something know that s foolish truly taste weird nasty unappeale unsweetened tea earth that s say lot company put cheap product foul simply have not start quality ingredient foul foul nasty foul smell like sweet rancid smell gym full sweaty high school athlete plain yuck'] \n",
      "\n",
      "['Positivo', 'perfect', 'need mother wedding cake rather worried move date perfect they re pretty much ms rock form wonderfull delivery super speedy big plus little time get mom wedding cake together'] \n",
      "\n",
      "['Positivo', 'yummy', 'buy several different flavor plum organic pouch really like product screw lid back give portion baby put back fridge later lid seal back use baby young do not want eat whole pouch one meal old hand whole pouch snack able get food good flow fast stuffing face slow get frustrated month old hold suck also mess mess least baby have not spill drop also like ingredient product feel like give healthy fruit snack'] \n",
      "\n",
      "['Positivo', 'delicious', 'one eat dry fruit get pack free sit counter week finally decide try dry mango taste good like candy chewy still edible eat lot slice one sit feel sick eat time save later maximum enjoyment'] \n",
      "\n",
      "['Positivo', 'great snack option', 'we ve eliminate many snack item health reason since we ve popcorn hand we ve enjoy several time week unlike we ve try pop well every time do not end two inch unpopped kernal bottom bowl taste great help keep we think less healthy snack long hand'] \n",
      "\n",
      "['Positivo', 'work great cost efficient', 'use buy oz jars bread machine yeast quite expensive last long make two pound loaf every three day afraid instant dry yeast would work way machine work great pound last lot keep well refrigerator really save product'] \n",
      "\n",
      "['Positivo', 'good chip cheese', 'good need cheese wish krinkle cut much prefer salt pepper krinkle cut well texture around brand much well frito lie good price amazon get free ship similar tim zack chip crispy side note hard bag open hand use knife scissor everytime'] \n",
      "\n",
      "['Positivo', 'excellent snack', 'item contain two box package taste fresh right size approximately calorie snack'] \n",
      "\n",
      "['Positivo', 'tabasco', 'price good delivery prompt deploy troop use pep mre send package home group'] \n",
      "\n",
      "['Positivo', 'amazing tea', 'good tea world help sleep night former drinker sleepy time choose'] \n",
      "\n",
      "['Positivo', 'barbecue favorite chip', 'popchip good potato chip ever hand bbq flavor strong tangy calorie fat low fry chip healthy least chip satisfy salt crunch craving problem truly addict could find sporadically local grocery find amazon start order case get save money boot chip like regular chip texture different assume popping taste become addicted one time favorite snack food'] \n",
      "\n",
      "['Positivo', 'good coffee', 'good coffee pod high quality traditional'] \n",
      "\n",
      "['Positivo', 'nice', 'bad good deal big bag food especially kind dog food bad salmon one cost alot'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range (20):\n",
    "    print(reviews[i],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22737227-2c5f-4c37-aa5c-55729a433038",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Transformación de datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc6d4d-9972-4a1c-aae4-671c02177f90",
   "metadata": {},
   "source": [
    "Con el fin de no cargar una y otra vez la normalizacion, guarde los resultados de la normalizacion en un csv, aqui es donde lo abrimos para su transformacion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b740ed1f-d4ba-494f-94fc-60fc6462777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getcwd()\n",
    "path = (directory+\"\\Reviews_140k.csv\")\n",
    "data_cleaned = leer_datos_noheader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35f779ad-d369-49a6-986e-7b8a69235ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40000, 40000, 40000]\n",
      "120000\n"
     ]
    }
   ],
   "source": [
    "reviews_compressed = []\n",
    "label_check = [0,0,0]\n",
    "for i in range(len(data_cleaned)):\n",
    "    if label_check[0] == label_check[1] == label_check[2] == 40000:\n",
    "        break\n",
    "    if data_cleaned[i][0] == 'Positivo' and label_check[0] < 40000:\n",
    "        reviews_compressed.append(data_cleaned[i])\n",
    "        label_check[0] += 1\n",
    "    elif data_cleaned[i][0] == 'Neutral' and label_check[1] < 40000:\n",
    "        reviews_compressed.append(data_cleaned[i])\n",
    "        label_check[1] += 1\n",
    "    elif data_cleaned[i][0] == 'Negativo' and label_check[2] < 40000:\n",
    "        reviews_compressed.append(data_cleaned[i])\n",
    "        label_check[2] += 1\n",
    "print(label_check)\n",
    "print(len(reviews_compressed))\n",
    "data_cleaned = reviews_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a36f64b-870d-4ba9-8dca-c4ea25ee2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for i in range(len(data_cleaned)):\n",
    "    text.append(data_cleaned[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "731220a0-fa3f-44ff-b1a5-1b39e3917ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "treat perfect disguise hide daily med little furball plus since cost bag store price low shipping fee bargain thank bunch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(text))\n",
    "print(text[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc06382-101c-4cc0-9371-153dd6f25e43",
   "metadata": {},
   "source": [
    "Realizamos la vectorizacion de los datos tomando la lista de los textos de las reseñas, en el caso de tfidf, ademas printeo el tamaño de palabras unicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e396aa4-56e7-4c00-847a-9d988072396d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de Palabras únicas: 52666\n"
     ]
    }
   ],
   "source": [
    "text_tfidf = vectorizar_tfidf(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e0288270-3ed9-40ee-8263-2a4c76e6c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_onehot = vectorizar_onehot(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde6b818-a6f9-493c-9751-77e0e4de6112",
   "metadata": {},
   "source": [
    "Imprimimos una muestra de como se observa TF-IDF y one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11eb99af-b993-438d-a4df-143bd0a9c690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6283)\t0.10205401103059528\n",
      "  (0, 11610)\t0.3938934447094808\n",
      "  (0, 19528)\t0.1808582127790854\n",
      "  (0, 45350)\t0.2651735766030554\n",
      "  (0, 28161)\t0.1740201202973761\n",
      "  (0, 30008)\t0.12044278393722233\n",
      "  (0, 32686)\t0.19583745738826608\n",
      "  (0, 41030)\t0.18740770597476783\n",
      "  (0, 41842)\t0.21110333456364072\n",
      "  (0, 33202)\t0.20836071316587587\n",
      "  (0, 34655)\t0.18527679985191514\n",
      "  (0, 42989)\t0.22629114362711517\n",
      "  (0, 10500)\t0.22972949168440762\n",
      "  (0, 8210)\t0.15992547972496238\n",
      "  (0, 43948)\t0.17814847807432735\n",
      "  (0, 41069)\t0.29074072172372506\n",
      "  (0, 31580)\t0.23557272850227207\n",
      "  (0, 4830)\t0.19064477000743224\n",
      "  (0, 29336)\t0.15986094774869394\n",
      "  (0, 47609)\t0.3087146886722414\n",
      "  (0, 45723)\t0.17974911559001644\n"
     ]
    }
   ],
   "source": [
    "print(text_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d91b2be-b571-492b-9b5f-5a457e39ae27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 45723)\t1\n",
      "  (0, 47609)\t1\n",
      "  (0, 29336)\t1\n",
      "  (0, 4830)\t1\n",
      "  (0, 31580)\t1\n",
      "  (0, 41069)\t1\n",
      "  (0, 43948)\t1\n",
      "  (0, 8210)\t1\n",
      "  (0, 10500)\t1\n",
      "  (0, 42989)\t1\n",
      "  (0, 34655)\t1\n",
      "  (0, 33202)\t1\n",
      "  (0, 41842)\t1\n",
      "  (0, 41030)\t1\n",
      "  (0, 32686)\t1\n",
      "  (0, 30008)\t1\n",
      "  (0, 28161)\t1\n",
      "  (0, 45350)\t1\n",
      "  (0, 19528)\t1\n",
      "  (0, 11610)\t1\n",
      "  (0, 6283)\t1\n"
     ]
    }
   ],
   "source": [
    "print(text_onehot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecab80ae-081e-4619-bb59-47c41e983755",
   "metadata": {},
   "source": [
    "## **Analisis de Sentimientos mediante Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0059af9-8dc8-4095-b3e8-e04e0df314b8",
   "metadata": {},
   "source": [
    "Convertimos las etiquetas a una representacion en entero (0,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50a31a57-7c09-4371-a8cc-f333a665ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in range(len(data_cleaned)):\n",
    "    y.append(data_cleaned[i][0])\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b6b9e-23a2-49bd-baf9-3a3991dbf81b",
   "metadata": {},
   "source": [
    "Realizamos mediante sklearn la division de k-fold cross validation para tener 80% train y 20% test con un k de 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56c150ea-a519-444b-b4cd-4c97ade07ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = SVC(kernel='linear')\n",
    "logistic_reg = LogisticRegression(max_iter=1000)\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f60e3d-6c45-4172-82ee-c0c22794f82f",
   "metadata": {},
   "source": [
    "Finalmente, entrenamos cada uno de los modelos. Siendo en orden regresion logistica, SVM y arboles de desicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3235282-7ba7-4e6e-b892-d79825beed74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold: [0.72995833 0.726375   0.73283333 0.723375   0.72758333]\n",
      "Mean accuracy: 0.7280249999999999\n"
     ]
    }
   ],
   "source": [
    "# Realizar k-fold cross-validation y obtener los resultados de precisión\n",
    "scores = cross_val_score(clf, text_tfidf, y, cv=cv)\n",
    "\n",
    "# Imprimir los resultados de la cross-validation\n",
    "print(\"Accuracy for each fold:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ebb4af7-a49e-41c1-9f2e-b33bba078a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold: [0.72804167 0.72645833 0.73158333 0.72145833 0.72508333]\n",
      "Mean accuracy: 0.726525\n"
     ]
    }
   ],
   "source": [
    "scores2 = cross_val_score(logistic_reg, text_tfidf, y, cv=cv)\n",
    "\n",
    "print(\"Accuracy for each fold:\", scores2)\n",
    "print(\"Mean accuracy:\", scores2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d06d453-454e-4529-8e77-94aaa0eaf351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold: [0.651625   0.64758333 0.6505     0.65041667 0.64595833]\n",
      "Mean accuracy: 0.6492166666666666\n"
     ]
    }
   ],
   "source": [
    "scores3 = cross_val_score(decision_tree, text_tfidf, y, cv=cv)\n",
    "\n",
    "print(\"Accuracy for each fold:\", scores3)\n",
    "print(\"Mean accuracy:\", scores3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d571c7-4f98-4e3c-ac66-14072afd081c",
   "metadata": {},
   "source": [
    "## **Analisis de Sentimientos mediante diccionarios**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb37be79-32ab-40fc-954d-a7acb5f7e058",
   "metadata": {},
   "source": [
    "Llamamos a la funcion de ponderacion de polaridad de cada diccionario, guardando para cada reseña su prediccion con cada diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5561db15-41e3-4e6d-8e05-b9c98dbc8544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n"
     ]
    }
   ],
   "source": [
    "text_analysis = []\n",
    "hiv4 = ps.HIV4()\n",
    "\n",
    "for i in range(len(text)):\n",
    "    if i % 12000 == 0:\n",
    "        print(\"Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\")\n",
    "    harvard_pol_text = hiv4_polarity(text[i],hiv4)\n",
    "    nltk_pol_text = analyze_sentiment(text[i])\n",
    "    text_analysis.append([harvard_pol_text,nltk_pol_text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616281a5-e07d-4673-abf9-20534b90b6a1",
   "metadata": {},
   "source": [
    "Observamos las primeras 10 predicciones por cada diccionario, guardamos en un csv cada prediccion (para no cargar la prediccion una y otra vez) y finalmente lo cargamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7a3229c4-ece2-49a2-837c-261c80070642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Negativo', 'Positivo'], ['Positivo', 'Positivo'], ['Neutral', 'Positivo'], ['Neutral', 'Positivo'], ['Positivo', 'Positivo'], ['Neutral', 'Positivo'], ['Positivo', 'Positivo'], ['Neutral', 'Negativo'], ['Neutral', 'Positivo'], ['Neutral', 'Positivo']]\n"
     ]
    }
   ],
   "source": [
    "print(text_analysis[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "13169126-453b-40bc-903f-27a451124610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha guardado como text_analysis.csv correctamente.\n"
     ]
    }
   ],
   "source": [
    "filename = \"text_analysis.csv\"\n",
    "\n",
    "with open(filename, mode='w', newline='', encoding='utf-8') as archivo_csv:\n",
    "    writing = csv.writer(archivo_csv)\n",
    "\n",
    "    for fila in text_analysis:\n",
    "        writing.writerow(fila)\n",
    "\n",
    "print(f\"Se ha guardado como {filename} correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8a3c65f0-7a8c-4876-9bfc-a010fb187c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getcwd()\n",
    "path = (directory+\"\\\\text_analysis.csv\")\n",
    "txt_analysis = leer_datos_noheader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "afd41545-cac1-4558-ab35-0e8d973ba101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n"
     ]
    }
   ],
   "source": [
    "print(len(txt_analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb827f-8786-4128-96c7-c165ecfd3855",
   "metadata": {},
   "source": [
    "Ponderamos la presicion al evaluarlo con la etiqueta real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "88e3db37-4cbf-45ab-a0e8-61f9cbfb7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict = [0,0]\n",
    "for i in range(len(txt_analysis)):\n",
    "    if data_cleaned[i][0] == txt_analysis[i][0]:\n",
    "        accuracy_dict[0] += 1\n",
    "    if data_cleaned[i][0] == txt_analysis[i][1]:\n",
    "        accuracy_dict[1] += 1\n",
    "        \n",
    "accuracy_dict[0] = accuracy_dict[0] / 120000\n",
    "accuracy_dict[1] = accuracy_dict[1] / 120000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "61650e49-a036-41f1-984b-efe9eb083514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presicion usando diccionarios\n",
      "HIV4 ------------------ Opinion Lexicon\n",
      "[0.3845166666666667, 0.4150333333333333]\n"
     ]
    }
   ],
   "source": [
    "print(\"Presicion usando diccionarios\")\n",
    "print(\"HIV4 ------------------ Opinion Lexicon\")\n",
    "print(accuracy_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2ac07-5cfc-44fc-8e72-cf432646c40b",
   "metadata": {},
   "source": [
    "## **Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf1226-1282-41f7-9e3e-f96aab63a57d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Con capa de embeddings preconstruida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2dabc0-90fc-4dae-9e47-f3bd9d9ccad7",
   "metadata": {},
   "source": [
    "Preparamos la etiqueta y los datos para el embedding, padded_docs nos asegura que los documentos tengan un mismo tamaño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c904985b-36a3-4356-adc8-aac27f6ed007",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "y_cat = to_categorical(y, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(text)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "encoded_docs = t.texts_to_sequences(text)\n",
    "\n",
    "# Definimos el maximo size de cada documento\n",
    "max_length = 150\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6056e1e1-0444-4d3f-a018-c811f45db991",
   "metadata": {},
   "source": [
    "Cargamos GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4dd0b4b-76b7-4272-ac72-def22c7d2f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    " values = line.split()\n",
    " word = values[0]\n",
    " coefs = asarray(values[1:], dtype='float32')\n",
    " embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    " embedding_vector = embeddings_index.get(word)\n",
    " if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49951935-7805-4a2c-b08e-5cb061086a87",
   "metadata": {},
   "source": [
    "Creamos los splits para kfold, ademas de que creamos el modelo de la red neuronal usando el embedding de glove ya entrenado, aplicando los pesos de la matriz de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d46a556-1ebf-46b5-8a8d-d83b346a9408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K-Fold] Iteracion:  1\n",
      "WARNING:tensorflow:From C:\\Users\\crist\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\crist\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\crist\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\crist\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "0.5978749990463257\n",
      "[K-Fold] Iteracion:  2\n",
      "0.5911666750907898\n",
      "[K-Fold] Iteracion:  3\n",
      "0.5985000133514404\n",
      "[K-Fold] Iteracion:  4\n",
      "0.5980833172798157\n",
      "[K-Fold] Iteracion:  5\n",
      "0.5957083106040955\n",
      "Precision promedio: 59.63%\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "acc_scores = []\n",
    "\n",
    "iteracion = 0\n",
    "# Iteracion del modelo por KFolds\n",
    "for train_index, test_index in kf.split(padded_docs, y):\n",
    "    iteracion += 1\n",
    "    print(\"[K-Fold] Iteracion: \",iteracion)\n",
    "\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=150, trainable=False)\n",
    "    model.add(e)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(3, activation='softmax')) \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    X_train, X_test = padded_docs[train_index], padded_docs[test_index]\n",
    "    y_train, y_test = y_cat[train_index], y_cat[test_index]\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc_scores.append(accuracy)\n",
    "    print(accuracy)\n",
    "\n",
    "print(\"Precision promedio: {:.2f}%\".format(np.mean(acc_scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e61d09-4009-4c21-a2f0-b9dc291528c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Con capa de embeddings aprendida mediante el cuerpo de textos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce0b0e-365b-41ce-845e-3bf098c16e22",
   "metadata": {},
   "source": [
    "Realizamos el proceso de procesar nuestros documentos del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9af57146-bc1c-49e1-928b-df56512a436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "# integer encode the documents\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in text]\n",
    "#print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 100\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286eb6e-3e3d-4555-9ded-84eaaa89f6e0",
   "metadata": {},
   "source": [
    "En lugar de colocar la capa de embeddings precosntruida con GloVe, se aprendera mediante el cuerpo de textos, en este caso la dimensionalidad es menor aunque seguimos tomando el tamaño del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "126f4aa8-4f52-4cd3-b7eb-de9a309f9dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K-Fold] Iteracion:  1\n",
      "0.6744583249092102\n",
      "[K-Fold] Iteracion:  2\n",
      "0.6714583039283752\n",
      "[K-Fold] Iteracion:  3\n",
      "0.6805833578109741\n",
      "[K-Fold] Iteracion:  4\n",
      "0.6728749871253967\n",
      "[K-Fold] Iteracion:  5\n",
      "0.6694999933242798\n",
      "Precisión promedio: 67.38%\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "acc_scores = []\n",
    "\n",
    "iteracion = 0\n",
    "\n",
    "# Iteración del modelo por KFolds\n",
    "for train_index, test_index in kf.split(padded_docs, y):\n",
    "    iteracion += 1\n",
    "    print(\"[K-Fold] Iteracion: \", iteracion)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    X_train, X_test = padded_docs[train_index], padded_docs[test_index]\n",
    "    y_train, y_test = y_cat[train_index], y_cat[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc_scores.append(accuracy)\n",
    "    print(accuracy)\n",
    "\n",
    "print(\"Precisión promedio: {:.2f}%\".format(np.mean(acc_scores) * 100)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
