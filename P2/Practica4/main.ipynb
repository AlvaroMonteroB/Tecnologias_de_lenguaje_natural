{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montero Barraza Álvaro David\n",
    "\n",
    "Tecnologías de lenguaje natural 5BV1\n",
    "\n",
    "Ingeniería en IA\n",
    "\n",
    "Este programa identifica frases, palabras y documentos similares, recibe 10 documentos para generar el corpus\n",
    "\n",
    "Última modificación: 9/6/24\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modulos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/alvaromontero/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import random\n",
    "from keybert import KeyBERT\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import transformers\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Funciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(lista):\n",
    "    randomized = lista[:]  \n",
    "    random.shuffle(randomized)\n",
    "    return randomized\n",
    "\n",
    "# Elimina etiquetas HTML y comentarios dentro del propio texto como indicaciones extra entre parentesis o comentarios entre --\n",
    "def eliminar_etiquetas(text):\n",
    "    pattern = re.compile(r'<[^>]+>|\\([^)]+\\)|--[^\\-]+--')\n",
    "    doc = pattern.sub('', text)\n",
    "    return doc\n",
    "\n",
    "# Removemos apostrofes para quedarnos con palabras como cant couldnt o dont\n",
    "def remover_apostrofes(text):\n",
    "    pattern = r\"\\\\?'\"\n",
    "    doc = re.sub(pattern, '', text)\n",
    "    return doc\n",
    "# Nos quedamos con solo alfabeto eliminando caracteres especiales    \n",
    "def remover_especiales(text):\n",
    "    pattern = r\"[^a-zA-Z\\s.,]\"\n",
    "    doc = re.sub(pattern, ' ', text)\n",
    "    return doc\n",
    "# Quitamos dobles espacios para tener texto espaciado solamente por un espacio por palabra\n",
    "def dobles_espacios(text):\n",
    "    pattern = r'\\s+'\n",
    "    doc = re.sub(pattern, ' ', text)\n",
    "    return doc\n",
    "    \n",
    "# Removemos stop words unicamente en ingles\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "# Lematizacion mediante spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "def lemmatize(doc):\n",
    "    docs = list(nlp.pipe([doc]))\n",
    "    lemas = ' '.join([token.lemma_ for doc in docs for token in doc])\n",
    "    return lemas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Análisis exploratorio y preprocesamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Book_list=['/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica4/ancient_egypt.txt','/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica4/christ_roma.txt','/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica4/city_rome.txt',\n",
    "           '/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica4/early_european.txt','/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica4/EGYPT.txt','/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica4/greek_cities.txt',\n",
    "           '/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica4/greek_imp.txt','/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica4/Punic_war.txt',\n",
    "           '/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica4/religion_pagan.txt','/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica4/roman_religion.txt']\n",
    "#Book_list=[]\n",
    "\n",
    "Books_raw_file=[]\n",
    "for i in range(len(Book_list)):\n",
    "    with open(Book_list[i],'r',encoding='utf-8')as f:\n",
    "        file=f.read()\n",
    "        Books_raw_file.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo está configurado para usar CPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if spacy.prefer_gpu():\n",
    "    print(\"El modelo está configurado para usar GPU\")\n",
    "else:\n",
    "    print(\"El modelo está configurado para usar CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libro 0 5590 tokens  159 oraciones\n",
      "Libro 1 13995 tokens  436 oraciones\n",
      "Libro 2 1200 tokens  59 oraciones\n",
      "Libro 3 4270 tokens  222 oraciones\n",
      "Libro 4 14396 tokens  427 oraciones\n",
      "Libro 5 10223 tokens  300 oraciones\n",
      "Libro 6 9298 tokens  288 oraciones\n",
      "Libro 7 4234 tokens  170 oraciones\n",
      "Libro 8 8224 tokens  269 oraciones\n",
      "Libro 9 8213 tokens  267 oraciones\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp=spacy.load('en_core_web_trf')\n",
    "spacy.prefer_gpu()\n",
    "books=[]\n",
    "books_w_stops=[]\n",
    "books_w_process=[]\n",
    "for i,book in enumerate(Books_raw_file):\n",
    "    book=book.lower()\n",
    "    book=eliminar_etiquetas(book)\n",
    "    book=remover_apostrofes(book)\n",
    "    book=remover_especiales(book)\n",
    "    book=dobles_espacios(book)\n",
    "    \n",
    "    doc=nlp(book.lower())\n",
    "    books_w_process.append(book)\n",
    "    books_w_stops.append(doc.copy())#Libros con stopwords\n",
    "    \n",
    "    book=remove_stopwords(book)\n",
    "    doc_s=nlp(book)\n",
    "    \n",
    "    books.append(doc_s.copy())#Libros sin stopwords\n",
    "    print(f\"Libro {i} {len(doc)} tokens  {len(list(doc.sents))} oraciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin stopwords\n",
      "Libro 0 3212 tokens  180 oraciones\n",
      "Libro 1 7905 tokens  508 oraciones\n",
      "Libro 2 611 tokens  58 oraciones\n",
      "Libro 3 2598 tokens  226 oraciones\n",
      "Libro 4 7993 tokens  451 oraciones\n",
      "Libro 5 5847 tokens  309 oraciones\n",
      "Libro 6 5026 tokens  314 oraciones\n",
      "Libro 7 2311 tokens  183 oraciones\n",
      "Libro 8 4563 tokens  277 oraciones\n",
      "Libro 9 4492 tokens  286 oraciones\n"
     ]
    }
   ],
   "source": [
    "print(\"Sin stopwords\")\n",
    "for i,book in enumerate(books):\n",
    "    print(f\"Libro {i} {len(book)} tokens  {len(list(book.sents))} oraciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Desarrollo de la práctica**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similitud de palabras con synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ\n",
      "ADJ\n",
      "ADJ\n",
      "ADJ\n",
      "PROPN\n",
      "NOUN\n",
      "ADV\n",
      "PROPN\n",
      "ADJ\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "for i,book in enumerate(books):\n",
    "    print(book[10].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_verb(doc):\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    if not verbs:\n",
    "        return None\n",
    "    most_common = Counter(verbs).most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "def most_common_verb_trf(doc):#Uso con transformers\n",
    "    doc=nlp(doc)\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    if not verbs:\n",
    "        return None\n",
    "    most_common = Counter(verbs).most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "def most_common_noun(doc):\n",
    "    nouns = [token.lemma_ for token in doc if token.pos_=='NOUN']\n",
    "    #print(len(nouns))\n",
    "    if not nouns:\n",
    "        print(\"No hubo\")\n",
    "        return None\n",
    "    most_common = Counter(nouns).most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "\n",
    "def most_common_terrm(doc):\n",
    "    nouns = [token.lemma_ for token in doc if  token.pos_ !='PUNCT']\n",
    "    #print(len(nouns))\n",
    "    if not nouns:\n",
    "        print(\"No hubo\")\n",
    "        return None\n",
    "    most_common = Counter(nouns).most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "def most_common_term_trf(doc):#Uso con transformers\n",
    "    doc=nlp(doc)\n",
    "    nouns = [token.lemma_ for token in doc if  token.pos_ !='PUNCT']\n",
    "    #print(len(nouns))\n",
    "    if not nouns:\n",
    "        print(\"No hubo\")\n",
    "        return None\n",
    "    most_common = Counter(nouns).most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "def get_synsets(word, lang='eng'):\n",
    "    return wn.synsets(word, lang=lang, pos=wn.VERB)\n",
    "\n",
    "def find_similar_terms(target, tokens, similarity_func):\n",
    "    target_synsets = get_synsets(target)\n",
    "    if not target_synsets:\n",
    "        return []\n",
    "    target_synset = target_synsets[0]\n",
    "    \n",
    "    similarities = []\n",
    "    for verb in tokens:\n",
    "        verb_synsets = get_synsets(verb)\n",
    "        if not verb_synsets:\n",
    "            continue\n",
    "        verb_synset = verb_synsets[0]\n",
    "        similarity = similarity_func(target_synset, verb_synset)\n",
    "        if similarity is not None:\n",
    "            similarities.append((verb, similarity))\n",
    "    \n",
    "    # Ordenar por similitud y obtener los 5 más similares\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return [verb for verb, _ in similarities[:5]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud de documentos con Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libro 0\n",
      "Term: give     wup:  ['love', 'fruit', 'prevent', 'indulge', 'bore']\n",
      "Term: give     path: ['love', 'fruit', 'prevent', 'indulge', 'bore']\n",
      "Libro 1\n",
      "Term: find     wup:  ['love', 'own', 'fruit', 'determined', 'base']\n",
      "Term: find     path: ['love', 'own', 'fruit', 'determined', 'base']\n",
      "Libro 2\n",
      "Term: see     wup:  ['use', 'understanding', 'get', 'begin', 'lost']\n",
      "Term: see     path: ['hear', 'sense', 'use', 'understanding', 'get']\n",
      "Libro 3\n",
      "Term: find     wup:  ['use', 'want', 'love', 'include', 'gather']\n",
      "Term: find     path: ['use', 'want', 'love', 'include', 'gather']\n",
      "Libro 4\n",
      "Term: find     wup:  ['love', 'base', 'ruin', 'prevent', 'exist']\n",
      "Term: find     path: ['love', 'base', 'ruin', 'prevent', 'exist']\n",
      "Libro 5\n",
      "Term: find     wup:  ['love', 'base', 'ruin', 'prevent', 'indulge']\n",
      "Term: find     path: ['love', 'base', 'ruin', 'prevent', 'indulge']\n",
      "Libro 6\n",
      "Term: make     wup:  ['do', 'love', 'own', 'lack', 'base']\n",
      "Term: make     path: ['do', 'love', 'own', 'lack', 'base']\n",
      "Libro 7\n",
      "Term: make     wup:  ['do', 'use', 'want', 'enter', 'hold']\n",
      "Term: make     path: ['do', 'use', 'want', 'enter', 'hold']\n",
      "Libro 8\n",
      "Term: worship     wup:  ['adore', 'love', 'fruit', 'base', 'exist']\n",
      "Term: worship     path: ['adore', 'love', 'fruit', 'base', 'exist']\n",
      "Libro 9\n",
      "Term: say     wup:  ['state', 'tell', 'declare', 'answer', 'sum']\n",
      "Term: say     path: ['state', 'tell', 'declare', 'answer', 'sum']\n"
     ]
    }
   ],
   "source": [
    "#Terminos similares\n",
    "for i,book in enumerate(books):\n",
    "    most_common=most_common_verb(book)\n",
    "    tokens = [token.lemma_ for token in book]#Lista de los tokens\n",
    "    tokens=list(set(tokens))#Eliminamos repetidos\n",
    "    tokens.remove(most_common)#Removemos para que no se compara con sí mismo\n",
    "    similar_verbs_wup = find_similar_terms(most_common, tokens, wn.wup_similarity)#Usando wup\n",
    "    similar_verbs_path = find_similar_terms(most_common, tokens, wn.path_similarity)#Usando path\n",
    "    print(f'Libro {i}')\n",
    "    print(f\"Term: {most_common}     wup:  {similar_verbs_wup}\")\n",
    "    print(f\"Term: {most_common}     path: {similar_verbs_path}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libro 0\n",
      "verb: give     wup:  ['get', 'arrive', 'precede', 'start', 'spread']\n",
      "verb: give     path: ['get', 'arrive', 'precede', 'start', 'spread']\n",
      "Libro 1\n",
      "verb: find     wup:  ['shame', 'own', 'allow', 'create', 'study']\n",
      "verb: find     path: ['shame', 'own', 'allow', 'create', 'study']\n",
      "Libro 2\n",
      "verb: see     wup:  ['give', 'get', 'keep', 'know', 'disappear']\n",
      "verb: see     path: ['hear', 'give', 'get', 'keep', 'know']\n",
      "Libro 3\n",
      "verb: find     wup:  ['start', 'spread', 'study', 'vary', 'base']\n",
      "verb: find     path: ['start', 'spread', 'study', 'vary', 'base']\n",
      "Libro 4\n",
      "verb: find     wup:  ['dominate', 'utilize', 'allow', 'end', 'study']\n",
      "verb: find     path: ['dominate', 'utilize', 'allow', 'end', 'study']\n",
      "Libro 5\n",
      "verb: find     wup:  ['allow', 'meet', 'hold', 'connect', 'go']\n",
      "verb: find     path: ['allow', 'meet', 'hold', 'connect', 'go']\n",
      "Libro 6\n",
      "verb: make     wup:  ['do', 'enunciate', 'get', 'dominate', 'own']\n",
      "verb: make     path: ['do', 'enunciate', 'get', 'dominate', 'own']\n",
      "Libro 7\n",
      "verb: make     wup:  ['do', 'reach', 'get', 'arrive', 'control']\n",
      "verb: make     path: ['do', 'reach', 'get', 'arrive', 'control']\n",
      "Libro 8\n",
      "verb: worship     wup:  ['adore', 'get', 'allow', 'create', 'signify']\n",
      "verb: worship     path: ['adore', 'love', 'get', 'allow', 'create']\n",
      "Libro 9\n",
      "verb: say     wup:  ['tell', 'declare', 'sum', 'promise', 'suggest']\n",
      "verb: say     path: ['tell', 'declare', 'sum', 'promise', 'suggest']\n"
     ]
    }
   ],
   "source": [
    "#Verbos similares\n",
    "for i,book in enumerate(books):\n",
    "    most_common=most_common_verb(book)\n",
    "    tokens = [token.lemma_ for token in book if token.pos_=='VERB']#Lista de los tokens\n",
    "    tokens=list(set(tokens))#Eliminamos repetidos\n",
    "    tokens.remove(most_common)#Removemos para que no se compara con sí mismo\n",
    "    similar_verbs_wup = find_similar_terms(most_common, tokens, wn.wup_similarity)#Usando wup\n",
    "    similar_verbs_path = find_similar_terms(most_common, tokens, wn.path_similarity)#Usando path\n",
    "    print(f'Libro {i}')\n",
    "    print(f\"verb: {most_common}     wup:  {similar_verbs_wup}\")\n",
    "    print(f\"verb: {most_common}     path: {similar_verbs_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libro 0  989\n",
      "libro 1  1869\n",
      "libro 2  194\n",
      "libro 3  931\n",
      "libro 4  2090\n",
      "libro 5  1280\n",
      "libro 6  1559\n",
      "libro 7  680\n",
      "libro 8  1318\n",
      "libro 9  1326\n"
     ]
    }
   ],
   "source": [
    "for i,book in enumerate(books):\n",
    "    tokens = [token.lemma_ for token in book if token.pos_=='NOUN']\n",
    "    print(f'libro {i}  {len(tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libro 0\n",
      "Noun: land     wup:  ['end', 'hold', 'travel', 'spread', 'feature']\n",
      "Noun: land     path: ['end', 'hold', 'travel', 'spread', 'feature']\n",
      "Libro 1\n",
      "Noun: name     wup:  ['call', 'title', 'understanding', 'stag', 'end']\n",
      "Noun: name     path: ['call', 'title', 'understanding', 'stag', 'end']\n",
      "Libro 2\n",
      "Noun: man     wup:  ['understanding', 'end', 'fighting', 'lack', 'ruin']\n",
      "Noun: man     path: ['understanding', 'end', 'fighting', 'lack', 'ruin']\n",
      "Libro 3\n",
      "Noun: illustration     wup:  []\n",
      "Noun: illustration     path: []\n",
      "Libro 4\n",
      "Noun: period     wup:  []\n",
      "Noun: period     path: []\n",
      "Libro 5\n",
      "Noun: foot     wup:  ['supply', 'portion', 'power', 'gate', 'bottom']\n",
      "Noun: foot     path: ['supply', 'portion', 'power', 'hand', 'gate']\n",
      "Libro 6\n",
      "Noun: city     wup:  []\n",
      "Noun: city     path: []\n",
      "Libro 7\n",
      "Noun: ship     wup:  ['meet', 'support', 'use', 'encounter', 'practice']\n",
      "Noun: ship     path: ['herd', 'place', 'transport', 'meet', 'port']\n",
      "Libro 8\n",
      "Noun: religion     wup:  []\n",
      "Noun: religion     path: []\n",
      "Libro 9\n",
      "Noun: man     wup:  ['end', 'study', 'hold', 'move', 'travel']\n",
      "Noun: man     path: ['slave', 'labour', 'toil', 'farm', 'end']\n"
     ]
    }
   ],
   "source": [
    "#Sustantivo\n",
    "for i,book in enumerate(books):\n",
    "    most_common=most_common_noun(book)\n",
    "    tokens = [token.lemma_ for token in book if token.pos_=='NOUN']#Lista de los tokens\n",
    "    tokens=list(set(tokens))#Eliminamos repetidos\n",
    "    tokens.remove(most_common)#Removemos para que no se compara con sí mismo\n",
    "    similar_nouns_wup = find_similar_terms(most_common, tokens, wn.wup_similarity)#Usando wup\n",
    "    similar_nouns_path = find_similar_terms(most_common, tokens, wn.path_similarity)#Usando path\n",
    "    print(f'Libro {i}')\n",
    "    print(f\"Noun: {most_common}     wup:  {similar_nouns_wup}\")\n",
    "    print(f\"Noun: {most_common}     path: {similar_nouns_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud de documentos con Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('shape egypt like lily', 0.6914)\n",
      "('transformation rome pagan christian', 0.7486)\n",
      "('civilized roman people ruled', 0.694)\n",
      "('egyptians kind paper papyrus', 0.679)\n",
      "('later age prehistoric egypt', 0.7529)\n",
      "('turkom shepherds end asia', 0.61)\n",
      "('policy imperialism deal greeks', 0.6834)\n",
      "('regulus advanced carthage conquering', 0.7003)\n",
      "('begin description ancient religions', 0.6764)\n",
      "('cicero plancus despair war', 0.6379)\n"
     ]
    }
   ],
   "source": [
    "#Extracción de oraciones clave\n",
    "key_phrases=[]\n",
    "key_model=KeyBERT()\n",
    "for book in books_w_process:#Libros con stopwords\n",
    "    key_phrase=key_model.extract_keywords(book,keyphrase_ngram_range=(4,4),nr_candidates=1)\n",
    "    key=key_phrase[0]\n",
    "    key_phrases.append(key[0])\n",
    "    print(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_similarity(synsets1, synsets2, similarity_func):\n",
    "    max_sim = 0\n",
    "    for synset1, synset2 in product(synsets1, synsets2):\n",
    "        sim = similarity_func(synset1, synset2)\n",
    "        if sim is not None and sim > max_sim:\n",
    "            max_sim = sim\n",
    "    return max_sim\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2, similarity_func):\n",
    "    #lematizar las frases\n",
    "    \n",
    "    synsets1 = [get_synsets(token.lemma_) for token in sentence1 if token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV']]\n",
    "    synsets2 = [get_synsets(token.lemma_) for token in sentence2 if token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV']]\n",
    "    \n",
    "    # Filtrar palabras sin synsets\n",
    "    synsets1 = [syn for syn in synsets1 if syn]\n",
    "    synsets2 = [syn for syn in synsets2 if syn]\n",
    "    \n",
    "    if not synsets1 or not synsets2:\n",
    "        return 0\n",
    "\n",
    "    # Calcular las similitudes entre los pares de synsets y promediar\n",
    "    total_sim = 0\n",
    "    count = 0\n",
    "    for synset_group1 in synsets1:\n",
    "        for synset_group2 in synsets2:\n",
    "            sim = max_similarity(synset_group1, synset_group2, similarity_func)\n",
    "            if sim:\n",
    "                total_sim += sim\n",
    "                count += 1\n",
    "    \n",
    "    return total_sim / count if count else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin description ancient religions\n",
      "Con: shape egypt like lily = 0.25\n",
      "Con: transformation rome pagan christian = 0\n",
      "Con: civilized roman people ruled = 0.2611111111111111\n",
      "Con: egyptians kind paper papyrus = 0.25\n",
      "Con: later age prehistoric egypt = 0.25\n",
      "Con: turkom shepherds end asia = 0.23809523809523808\n",
      "Con: policy imperialism deal greeks = 0.3333333333333333\n",
      "Con: regulus advanced carthage conquering = 0.29166666666666663\n",
      "Con: cicero plancus despair war = 0.29166666666666663\n"
     ]
    }
   ],
   "source": [
    "for phrase in key_phrases:\n",
    "    phrase=nlp(phrase)\n",
    "Prefered_phrase=key_phrases[8]#quitamos la 8va frase \n",
    "key_phrases.remove(Prefered_phrase)\n",
    "print(Prefered_phrase)\n",
    "for i,phrase in enumerate(key_phrases):\n",
    "    path_sim = sentence_similarity(Prefered_phrase, phrase, wn.path_similarity)\n",
    "    print(f\"Con: {phrase} = {path_sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud sintáctica de palabras y documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 vectores almacenados\n"
     ]
    }
   ],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica4/glove.6B/glove.6B.200d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "print(f\"{len(embeddings_dict)} vectores almacenados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding,embedding_dict):#Embedding de la palabra y el diccionario que escogimos\n",
    "    return sorted(embedding_dict.keys(), key=lambda word: spatial.distance.cosine(embedding_dict[word], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libro 0\n",
      "Noun: give     Closest:  [(0.1593226107299469, 'take'), (0.18839561748695555, 'make'), (0.23757285277657259, 'come'), (0.26393991767803815, 'go'), (0.279018728763182, 'allow')]\n",
      "Libro 1\n",
      "Noun: find     Closest:  [(0.22592145445329803, 'able'), (0.24292740111803524, 'make'), (0.2438174501181356, 'come'), (0.2535970465454649, 'know'), (0.2563319111105645, 'help')]\n",
      "Libro 2\n",
      "Noun: live     Closest:  [(0.4271273547596577, 'people'), (0.44265209369391645, 'come'), (0.457291898275211, 'rest'), (0.4807647972238607, 'life'), (0.48460927543866095, 'go')]\n",
      "Libro 3\n",
      "Noun: find     Closest:  [(0.22592145445329803, 'able'), (0.24292740111803524, 'make'), (0.2438174501181356, 'come'), (0.2535970465454649, 'know'), (0.2584960495438203, 'way')]\n",
      "Libro 4\n",
      "Noun: find     Closest:  [(0.22592145445329803, 'able'), (0.24292740111803524, 'make'), (0.2438174501181356, 'come'), (0.2535970465454649, 'know'), (0.2563319111105645, 'help')]\n",
      "Libro 5\n",
      "Noun: find     Closest:  [(0.22592145445329803, 'able'), (0.24292740111803524, 'make'), (0.2438174501181356, 'come'), (0.2535970465454649, 'know'), (0.2563319111105645, 'help')]\n",
      "Libro 6\n",
      "Noun: give     Closest:  [(0.1593226107299469, 'take'), (0.18839561748695555, 'make'), (0.22082891579456843, 'get'), (0.22977822998405828, 'need'), (0.23656673842275433, 'put')]\n",
      "Libro 7\n",
      "Noun: come     Closest:  [(0.14209417431965976, 'go'), (0.1738181291101193, 'take'), (0.17627846505916922, 'want'), (0.177071470200178, 'make'), (0.18587417877081736, 'get')]\n",
      "Libro 8\n",
      "Noun: worship     Closest:  [(0.37660586111372807, 'religious'), (0.3788677537994838, 'prayer'), (0.38232277474884113, 'god'), (0.41237261118668755, 'ritual'), (0.41341884424263764, 'deity')]\n",
      "Libro 9\n",
      "Noun: say     Closest:  [(0.1826135992328587, 'know'), (0.2217442302704824, 'think'), (0.2336177454296493, 'come'), (0.26104881991243845, 'sure'), (0.26118168616139825, 'reason')]\n"
     ]
    }
   ],
   "source": [
    "for i,book in enumerate(books):\n",
    "    book=[token for token in book if not token.is_stop and not token.is_punct]\n",
    "    most_common=most_common_verb(book)\n",
    "    tokens = [token.lemma_ for token in book if not token.is_punct]#Lista de los tokens\n",
    "    tokens=list(set(tokens))#Eliminamos repetidos\n",
    "    if most_common in tokens:\n",
    "        tokens.remove(most_common)#Removemos para que no se compara con sí mismo\n",
    "    embeddings_list=[(token,embeddings_dict[token]) for token in tokens if token in embeddings_dict]#Solo ponemos los tokens que aparecen en el texto\n",
    "    similar_terms=find_closest_embeddings(embeddings_dict[most_common],dict(embeddings_list))[:5]\n",
    "    similar_terms=[(spatial.distance.cosine(embeddings_dict[token],embeddings_dict[most_common]) ,token)for token in similar_terms]\n",
    "    print(f'Libro {i}')\n",
    "    print(f\"Noun: {most_common}     Closest:  {similar_terms}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    vec2=vec2.T\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libro 0\n",
      "Term: give     Closest:  [(tensor([0.9558]), 'giving'), (tensor([0.9352]), 'take'), (tensor([0.9199]), 'tell'), (tensor([0.9182]), 'make'), (tensor([0.9171]), 'spread')]\n",
      "Libro 1\n",
      "Term: find     Closest:  [(tensor([0.9009]), 'finding'), (tensor([0.8920]), 'searching'), (tensor([0.8894]), 'go'), (tensor([0.8893]), 'read'), (tensor([0.8878]), 'help')]\n",
      "Libro 2\n",
      "Term: see     Closest:  [(tensor([0.9091]), 'explains'), (tensor([0.9030]), 'respects'), (tensor([0.8977]), 'understand'), (tensor([0.8935]), 'swept'), (tensor([0.8929]), 'due')]\n",
      "Libro 3\n",
      "Term: find     Closest:  [(tensor([0.9081]), 'reach'), (tensor([0.8894]), 'go'), (tensor([0.8847]), 'learn'), (tensor([0.8788]), 'take'), (tensor([0.8770]), 'give')]\n",
      "Libro 4\n",
      "Term: find     Closest:  [(tensor([0.9081]), 'reach'), (tensor([0.9025]), 'hunt'), (tensor([0.8905]), 'seek'), (tensor([0.8894]), 'go'), (tensor([0.8893]), 'read')]\n",
      "Libro 5\n",
      "Term: find     Closest:  [(tensor([0.9081]), 'reach'), (tensor([0.8976]), 'save'), (tensor([0.8878]), 'help'), (tensor([0.8847]), 'learn'), (tensor([0.8844]), 'know')]\n",
      "Libro 6\n",
      "Term: have     Closest:  [(tensor([0.9264]), 'upon'), (tensor([0.9195]), 'reach'), (tensor([0.9189]), 'justify'), (tensor([0.9167]), 'appreciate'), (tensor([0.9167]), 'gave')]\n",
      "Libro 7\n",
      "Term: make     Closest:  [(tensor([0.9570]), 'making'), (tensor([0.9559]), 'makes'), (tensor([0.9390]), 'prevent'), (tensor([0.9369]), 'deliver'), (tensor([0.9364]), 'allow')]\n",
      "Libro 8\n",
      "Term: have     Closest:  [(tensor([0.9284]), 'get'), (tensor([0.9264]), 'upon'), (tensor([0.9192]), 'seem'), (tensor([0.9167]), 'gave'), (tensor([0.9159]), 'believe')]\n",
      "Libro 9\n",
      "Term: have     Closest:  [(tensor([0.9284]), 'get'), (tensor([0.9264]), 'upon'), (tensor([0.9261]), 'allow'), (tensor([0.9209]), 'rags'), (tensor([0.9195]), 'reach')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model=transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer=transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "for i,book in enumerate(books_w_process):\n",
    "    #book=[token for token in book if not token.is_stop and not token.is_punct]\n",
    "    most_common=most_common_verb_trf(book)\n",
    "    \n",
    "    encoded_term=tokenizer(most_common,return_tensors='pt',truncation=True,max_length=512)\n",
    "    #encoded_book=tokenizer(book,return_tensors='pt',truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        term_outputs = model(**encoded_term).last_hidden_state.mean(dim=1)\n",
    "        #book_outputs = model(**encoded_book).last_hidden_state.mean(dim=1)\n",
    "        \n",
    "    similarities = []\n",
    "    \"\"\"\n",
    "    for j,term in enumerate(book_outputs):\n",
    "        #print(term_outputs.last_hidden_state[0].shape)\n",
    "        #print(term.shape)\n",
    "        similarity = torch.cosine_similarity(term_outputs,term)\n",
    "        similarities.append((similarity,tokenizer.decode(term)))\"\"\"\n",
    "        \n",
    "    \n",
    "    tokens = [token.text for token in books[i] if not token.is_punct]#Lista de palabras\n",
    "    tokens=list(set(tokens))#Eliminamos repetidos\n",
    "    if most_common in tokens:\n",
    "        tokens.remove(most_common)#Removemos para que no se compara con sí mismo\n",
    "    for term in tokens:\n",
    "        term_aux_tsr=tokenizer(term,return_tensors='pt',truncation=True,max_length=512)#Tokenizamos cada palabra\n",
    "        with torch.no_grad():\n",
    "            term_aux_hidden=model(**term_aux_tsr).last_hidden_state.mean(dim=1)#Obtenemos el embedding\n",
    "        similarity=torch.cosine_similarity(term_outputs,term_aux_hidden)#Calculamos similitud coseno\n",
    "        similarities.append((similarity,term))\n",
    "        \n",
    "        \n",
    "    #print(len(similarities))\n",
    "    sorted_indices = sorted(similarities,key=lambda x:x[0],reverse=True)\n",
    "    #similar_terms = [(tokenizer.decode(encoded_book['input_ids'][i]), similarities[i]) for i in sorted_indices[:5]]\n",
    "    similar_terms=sorted_indices[:5]\n",
    "    \n",
    "    print(f'Libro {i}')\n",
    "    print(f\"Term: {most_common}     Closest:  {similar_terms}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
