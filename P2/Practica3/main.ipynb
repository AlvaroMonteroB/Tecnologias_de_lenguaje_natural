{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/alvaromontero/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spacy english en_core_web_trf\n",
    "\n",
    "#Librerias a usar\n",
    "import nltk\n",
    "from bert_serving.client import BertClient\n",
    "import spacy\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar libros\n",
    "Book_list=[\"/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica3/divine_comedy.txt\",'/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica3/Oddysey.txt'\n",
    "           ,'/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica3/Illiad.txt','/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica3/Great_Gatsby.txt',\n",
    "           '/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica3/Tom_Sawyer.txt','/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica3/Wuthering_Heights.txt']\n",
    "\n",
    "\n",
    "Books_raw_file=[]\n",
    "for i in range(len(Book_list)):\n",
    "    with open(Book_list[i],'r',encoding='utf-8')as f:\n",
    "        file=f.read()\n",
    "        Books_raw_file.append(file)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESAMIENTO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones\n",
    "\n",
    "def eliminar_etiquetas(text):\n",
    "    pattern = re.compile(r'<[^>]+>|\\([^)]+\\)|--[^\\-]+--')\n",
    "    doc = pattern.sub('', text)\n",
    "    return doc\n",
    "\n",
    "# Removemos apostrofes para quedarnos con palabras como cant couldnt o dont\n",
    "def remover_apostrofes(text):\n",
    "    pattern = r\"\\\\?'\"\n",
    "    doc = re.sub(pattern, '', text)\n",
    "    return doc\n",
    "# Nos quedamos con solo alfabeto eliminando caracteres especiales    \n",
    "def remover_especiales(text):\n",
    "    pattern = r\"[^a-z\\s]\"\n",
    "    doc = re.sub(pattern, ' ', text)\n",
    "    return doc\n",
    "# Quitamos dobles espacios para tener texto espaciado solamente por un espacio por palabra\n",
    "def dobles_espacios(text):\n",
    "    pattern = r'\\s+'\n",
    "    doc = re.sub(pattern, ' ', text)\n",
    "    return doc\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizacion tfidf\n",
    "def vectorizar_tfidf(corpus):\n",
    "    # TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # palabras únicas\n",
    "    feature_names_tfidf = vectorizer.get_feature_names_out()\n",
    "\n",
    "    print(\"Tamaño de Palabras únicas:\", len(feature_names_tfidf))\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitar inicio de texto\n",
    "deleted_lines_list=[134,336,1997,32,459,7]\n",
    "\n",
    "for i,book in enumerate(Books_raw_file):\n",
    "    lines = book.splitlines()  # Split the book text into lines\n",
    "    modified_lines = lines[deleted_lines_list[i]:]  # Keep lines starting from the specified index\n",
    "    modified_book = \"\\n\".join(modified_lines)  # Join the remaining lines back into a string\n",
    "    Books_raw_file[i]=modified_book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canto i\n",
      "\n",
      "\n",
      "in the midway of this our mortal life,\n",
      "i found me in a gloomy wood, astray\n",
      "gone from the path direct: and e’en to tell\n",
      "it were no easy task, how savage wild\n",
      "that forest, how robust and rough\n"
     ]
    }
   ],
   "source": [
    "for i,book in enumerate(Books_raw_file):\n",
    "    Books_raw_file[i]=book.lower()\n",
    "    \n",
    "print(Books_raw_file[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminacion de caracteres no importantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canto midway mortal life found gloomy wood as\n",
      "\n",
      "book gods council minerva visit ithaca challe\n",
      "\n",
      "iliad book argument contention achilles agame\n",
      "\n",
      "younger vulnerable years father gave advice t\n",
      "\n",
      "chapter tom answer tom answer gone boy wonder\n",
      "\n",
      "chapter returned visit landlord solitary neig\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "books_clean_files=[]\n",
    "books_clean_stops_file=[]\n",
    "for i,book in enumerate(Books_raw_file):\n",
    "    Text_aux=eliminar_etiquetas(book)\n",
    "    Text_aux=remover_apostrofes(Text_aux)\n",
    "    Text_aux=dobles_espacios(Text_aux)\n",
    "    Text_aux=remover_especiales(Text_aux)\n",
    "    \n",
    "    books_clean_stops_file.append(deepcopy(Text_aux))#Por si el modelo no necesita quitar stopwords\n",
    "    \n",
    "    Text_aux=remove_stopwords(Text_aux)\n",
    "    books_clean_files.append(Text_aux)#Texto sin stopwords\n",
    "    print(Text_aux[:45]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracción de oraciones clave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de Palabras únicas: 24430\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF NLTK\n",
    "tfidf_matrix=vectorizar_tfidf(books_clean_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16134)\t0.000573076668833097\n",
      "  (0, 9211)\t0.0009659803341068473\n",
      "  (0, 23317)\t0.0007921176925078543\n",
      "  (0, 16168)\t0.0009659803341068473\n",
      "  (0, 19689)\t0.0009659803341068473\n",
      "  (0, 6644)\t0.0009659803341068473\n",
      "  (0, 17032)\t0.0013375203702983554\n",
      "  (0, 3840)\t0.0009659803341068473\n",
      "  (0, 16946)\t0.0007921176925078543\n",
      "  (0, 5417)\t0.0007921176925078543\n",
      "  (0, 5226)\t0.0006687601851491777\n",
      "  (0, 1117)\t0.0007921176925078543\n",
      "  (0, 12486)\t0.0006687601851491777\n",
      "  (0, 118)\t0.0006687601851491777\n",
      "  (0, 22588)\t0.0009659803341068473\n",
      "  (0, 22674)\t0.0009659803341068473\n",
      "  (0, 11285)\t0.0007921176925078543\n",
      "  (0, 978)\t0.0009659803341068473\n",
      "  (0, 15212)\t0.0009659803341068473\n",
      "  (0, 21130)\t0.0009659803341068473\n",
      "  (0, 8448)\t0.0006687601851491777\n",
      "  (0, 22937)\t0.0006687601851491777\n",
      "  (0, 21508)\t0.0007921176925078543\n",
      "  (0, 22186)\t0.0009659803341068473\n",
      "  (0, 1576)\t0.0007921176925078543\n",
      "  :\t:\n",
      "  (5, 6001)\t0.006587425108172797\n",
      "  (5, 6891)\t0.0193051135918628\n",
      "  (5, 9504)\t0.0480574104308074\n",
      "  (5, 5997)\t0.002370320210233074\n",
      "  (5, 24378)\t0.0431284452584169\n",
      "  (5, 5303)\t0.0193051135918628\n",
      "  (5, 7985)\t0.018894366494163595\n",
      "  (5, 2232)\t0.0010979041846954662\n",
      "  (5, 17552)\t0.011500918735577839\n",
      "  (5, 9768)\t0.0005489520923477331\n",
      "  (5, 18183)\t0.009955344882978912\n",
      "  (5, 24005)\t0.010679424540179422\n",
      "  (5, 18518)\t0.0037925123363729187\n",
      "  (5, 21283)\t0.0032937125540863986\n",
      "  (5, 6697)\t0.003285976781593668\n",
      "  (5, 21367)\t0.05011114591930344\n",
      "  (5, 6997)\t0.0010979041846954662\n",
      "  (5, 5910)\t0.0027447604617386657\n",
      "  (5, 15265)\t0.0041074709769920855\n",
      "  (5, 9498)\t0.020948101982659634\n",
      "  (5, 24168)\t0.0018962561681864594\n",
      "  (5, 9410)\t0.0014221921261398445\n",
      "  (5, 8759)\t0.01971586068956201\n",
      "  (5, 12558)\t0.022591090373456472\n",
      "  (5, 13859)\t0.0033184482943263037\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
