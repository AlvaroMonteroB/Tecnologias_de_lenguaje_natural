{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\diavl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\diavl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#spacy english en_core_web_trf\n",
    "\n",
    "#Librerias a usar\n",
    "\n",
    "#from bert_serving.client import BertClient\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#import torch; print(torch.__version__)\n",
    "import torch\n",
    "from transformers import BertModel,BertTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar libros\n",
    "#Book_list=[\"/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica3/divine_comedy.txt\",'/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica3/Oddysey.txt'\n",
    "#           ,'/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica3/Illiad.txt','/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica3/Great_Gatsby.txt','/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica3/Tom_Sawyer.txt','/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica3/Wuthering_Heights.txt']\n",
    "Book_list=[r\"C:\\Users\\diavl\\OneDrive\\Escritorio\\Repositorios\\Tecnologias_de_lenguaje_natural\\P2\\Practica3\\divine_comedy.txt\",r\"C:\\Users\\diavl\\OneDrive\\Escritorio\\Repositorios\\Tecnologias_de_lenguaje_natural\\P2\\Practica3\\Oddysey.txt\",r\"C:\\Users\\diavl\\OneDrive\\Escritorio\\Repositorios\\Tecnologias_de_lenguaje_natural\\P2\\Practica3\\Illiad.txt\",\n",
    "                r\"C:\\Users\\diavl\\OneDrive\\Escritorio\\Repositorios\\Tecnologias_de_lenguaje_natural\\P2\\Practica3\\Great_Gatsby.txt\",r\"C:\\Users\\diavl\\OneDrive\\Escritorio\\Repositorios\\Tecnologias_de_lenguaje_natural\\P2\\Practica3\\Tom_Sawyer.txt\",r\"C:\\Users\\diavl\\OneDrive\\Escritorio\\Repositorios\\Tecnologias_de_lenguaje_natural\\P2\\Practica3\\Wuthering_Heights.txt\"]\n",
    "\n",
    "Books_raw_file=[]\n",
    "for i in range(len(Book_list)):\n",
    "    with open(Book_list[i],'r',encoding='utf-8')as f:\n",
    "        file=f.read()\n",
    "        Books_raw_file.append(file)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESAMIENTO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones\n",
    "\n",
    "def eliminar_etiquetas(text):\n",
    "    pattern = re.compile(r'<[^>]+>|\\([^)]+\\)|--[^\\-]+--')\n",
    "    doc = pattern.sub('', text)\n",
    "    return doc\n",
    "\n",
    "# Removemos apostrofes para quedarnos con palabras como cant couldnt o dont\n",
    "def remover_apostrofes(text):\n",
    "    pattern = r\"\\\\?'\"\n",
    "    doc = re.sub(pattern, '', text)\n",
    "    return doc\n",
    "# Nos quedamos con solo alfabeto eliminando caracteres especiales    \n",
    "def remover_especiales(text):\n",
    "    pattern = r\"[^a-z\\s]\"\n",
    "    doc = re.sub(pattern, ' ', text)\n",
    "    return doc\n",
    "# Quitamos dobles espacios para tener texto espaciado solamente por un espacio por palabra\n",
    "def dobles_espacios(text):\n",
    "    pattern = r'\\s+'\n",
    "    doc = re.sub(pattern, ' ', text)\n",
    "    return doc\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizacion tfidf\n",
    "def vectorizar_tfidf(corpus):\n",
    "    # TF-IDF\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(4,4))\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # palabras únicas\n",
    "    feature_names_tfidf = vectorizer.get_feature_names_out()\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "    print(\"Tamaño de Palabras únicas:\", len(feature_names_tfidf))\n",
    "    return tfidf_matrix,vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitar inicio de texto\n",
    "deleted_lines_list=[134,336,1997,32,459,7]\n",
    "\n",
    "for i,book in enumerate(Books_raw_file):\n",
    "    lines = book.splitlines()  # Split the book text into lines\n",
    "    modified_lines = lines[deleted_lines_list[i]:]  # Keep lines starting from the specified index\n",
    "    modified_book = \"\\n\".join(modified_lines)  # Join the remaining lines back into a string\n",
    "    Books_raw_file[i]=modified_book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canto i\n",
      "\n",
      "\n",
      "in the midway of this our mortal life,\n",
      "i found me in a gloomy wood, astray\n",
      "gone from the path direct: and e’en to tell\n",
      "it were no easy task, how savage wild\n",
      "that forest, how robust and rough\n"
     ]
    }
   ],
   "source": [
    "for i,book in enumerate(Books_raw_file):\n",
    "    Books_raw_file[i]=book.lower()\n",
    "    \n",
    "print(Books_raw_file[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminacion de caracteres no importantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canto midway mortal life found gloomy wood as\n",
      "\n",
      "book gods council minerva visit ithaca challe\n",
      "\n",
      "iliad book argument contention achilles agame\n",
      "\n",
      "younger vulnerable years father gave advice t\n",
      "\n",
      "chapter tom answer tom answer gone boy wonder\n",
      "\n",
      "chapter returned visit landlord solitary neig\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "books_clean_files=[]\n",
    "books_clean_stops_file=[]\n",
    "for i,book in enumerate(Books_raw_file):\n",
    "    Text_aux=eliminar_etiquetas(book)\n",
    "    Text_aux=remover_apostrofes(Text_aux)\n",
    "    Text_aux=dobles_espacios(Text_aux)\n",
    "    Text_aux=remover_especiales(Text_aux)\n",
    "    \n",
    "    books_clean_stops_file.append(deepcopy(Text_aux))#Por si el modelo no necesita quitar stopwords\n",
    "    \n",
    "    Text_aux=remove_stopwords(Text_aux)\n",
    "    books_clean_files.append(Text_aux)#Texto sin stopwords\n",
    "    print(Text_aux[:45]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracción de oraciones clave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de Palabras únicas: 58074\n",
      "Frase: little ye shall see, Frecuencia: 0.008296684166811896\n",
      "Frase: phaeton ill knew guide, Frecuencia: 0.008296684166811896\n",
      "Frase: power one ask thou, Frecuencia: 0.008296684166811896\n",
      "Frase: speak whence ye stand, Frecuencia: 0.008296684166811896\n",
      "Frase: tell thou know st, Frecuencia: 0.008296684166811896\n",
      "Tamaño de Palabras únicas: 54649\n",
      "Frase: child morning rosy fingered, Frecuencia: 0.07648926028989292\n",
      "Frase: morning rosy fingered dawn, Frecuencia: 0.07648926028989292\n",
      "Frase: rosy fingered dawn appeared, Frecuencia: 0.07648926028989292\n",
      "Frase: soon enough eat drink, Frecuencia: 0.04428325595730643\n",
      "Frase: ulysses noble son laertes, Frecuencia: 0.04025750541573312\n",
      "Tamaño de Palabras únicas: 96575\n",
      "Frase: arm wield avenging bolt, Frecuencia: 0.009506006921700063\n",
      "Frase: arms resound broad buckler, Frecuencia: 0.009506006921700063\n",
      "Frase: conquest mighty jove decide, Frecuencia: 0.009506006921700063\n",
      "Frase: contend juno suit heavenly, Frecuencia: 0.009506006921700063\n",
      "Frase: daughter god whose arm, Frecuencia: 0.009506006921700063\n",
      "Tamaño de Palabras únicas: 23160\n",
      "Frase: ga od oh ga, Frecuencia: 0.019686209096093626\n",
      "Frase: od oh ga od, Frecuencia: 0.019686209096093626\n",
      "Frase: oh ga od oh, Frecuencia: 0.019686209096093626\n",
      "Frase: always watch longest day, Frecuencia: 0.013124139397395752\n",
      "Frase: beg pardon said mr, Frecuencia: 0.013124139397395752\n",
      "Tamaño de Palabras únicas: 34143\n",
      "Frase: black avenger spanish main, Frecuencia: 0.021609943726294125\n",
      "Frase: ting ling ling chow, Frecuencia: 0.016207457794720593\n",
      "Frase: anybody ever marry anybody, Frecuencia: 0.010804971863147063\n",
      "Frase: art thou dares hold, Frecuencia: 0.010804971863147063\n",
      "Frase: aunt polly sid mary, Frecuencia: 0.010804971863147063\n",
      "Tamaño de Palabras únicas: 54382\n",
      "Frase: seventy times seven times, Frecuencia: 0.012861448225731315\n",
      "Frase: began put questions concerning, Frecuencia: 0.00857429881715421\n",
      "Frase: common humanity sense duty, Frecuencia: 0.00857429881715421\n",
      "Frase: mr earnshaw invited called, Frecuencia: 0.00857429881715421\n",
      "Frase: often obliged seek kitchen, Frecuencia: 0.00857429881715421\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF NLTK\n",
    "\n",
    "for book in books_clean_files:#Iteramos sobre los libros para obtener lo mas representativo de cada texto\n",
    "    #tokens=word_tokenize(book)\n",
    "    #postags=pos_tag(tokens)\n",
    "    #stemmer=PorterStemmer()\n",
    "    #stemmed_tokens=[stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(4,4))#Vectorizador con 4 gramas\n",
    "    tfidf_matrix = vectorizer.fit_transform([book])\n",
    "    feature_names_tfidf = vectorizer.get_feature_names_out()\n",
    "    vocabulary = vectorizer.get_feature_names_out()#Obtenemos el bocabulario\n",
    "    print(\"Tamaño de Palabras únicas:\", len(feature_names_tfidf))\n",
    "    frecuencia_total = Counter()\n",
    "    for ngrama in vocabulary:#Contamos las frases mas comunes\n",
    "    # Obtiene el índice de la palabra en el vocabulario\n",
    "        indice = vectorizer.vocabulary_[ngrama]\n",
    "        # Suma las frecuencias de esa palabra en todas las frases\n",
    "        frecuencia_total[ngrama] = tfidf_matrix[:, indice].sum()\n",
    "    frases_mas_frecuentes = frecuencia_total.most_common(5)\n",
    "    for i in range(5):\n",
    "        print(f\"Frase: {frases_mas_frecuentes[i][0]}, Frecuencia: {frases_mas_frecuentes[i][1]}\")#El decimal es que tanto contribuyen al \"Corpus\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  2.- BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diavl\\miniconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name='bert-base-uncased'\n",
    "tokenizer=BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "#Tokenizar frases\n",
    "for book in books_clean_stops_file:#Requerimos el contexto completo\n",
    "    encoded_input = tokenizer([book],padding=True , truncation= True, return_tensors='pt')\n",
    "    input_ids = encoded_input['input_ids'].squeeze(0)\n",
    "    attention_mask = encoded_input['attention_mask'].squeeze(0)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs=model(input_ids,attention_mask=attention_mask)\n",
    "    last_hidden_state=outputs[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
