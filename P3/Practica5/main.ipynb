{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cabecera\n",
    "Montero Barraza Álvaro David\n",
    "\n",
    "5BV1\n",
    "\n",
    "Ingenieria en Inteligencia Artificial \n",
    "\n",
    "\n",
    "Práctica 5. Analisis de sentimientos\n",
    "\n",
    "A lo largo de esta practica, utilizaremos diversos modelos para el analisis de sentimientos, utilizando modelos de machine learning, diccionarios y redes neuronales. Ademas de que realizaremos un analisis de una coleccion de reseñas de comida fina en amazon y finalmente compararemos los distintos modelos al ponerlos a prueba con el dataset normalizado y vectorizado segun el modelo que corresponda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 00:36:27.220982: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-26 00:36:27.599989: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-26 00:36:28.271543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /home/alvaromontero/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/alvaromontero/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "import pysentiment2 as ps\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Funciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvaromontero/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/alvaromontero/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/alvaromontero/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "def randomize(lista):\n",
    "    randomized = lista[:]  \n",
    "    random.shuffle(randomized)\n",
    "    return randomized\n",
    "\n",
    "# Elimina etiquetas HTML y comentarios dentro del propio texto como indicaciones extra entre parentesis o comentarios entre --\n",
    "def eliminar_etiquetas(text):\n",
    "    pattern = re.compile(r'<[^>]+>|\\([^)]+\\)|--[^\\-]+--')\n",
    "    doc = pattern.sub('', text)\n",
    "    return doc\n",
    "\n",
    "# Removemos apostrofes para quedarnos con palabras como cant couldnt o dont\n",
    "def remover_apostrofes(text):\n",
    "    pattern = r\"\\\\?'\"\n",
    "    doc = re.sub(pattern, '', text)\n",
    "    return doc\n",
    "# Nos quedamos con solo alfabeto eliminando caracteres especiales    \n",
    "def remover_especiales(text):\n",
    "    pattern = r\"[^a-zA-Z\\s.,]\"\n",
    "    doc = re.sub(pattern, ' ', text)\n",
    "    return doc\n",
    "# Quitamos dobles espacios para tener texto espaciado solamente por un espacio por palabra\n",
    "def dobles_espacios(text):\n",
    "    pattern = r'\\s+'\n",
    "    doc = re.sub(pattern, ' ', text)\n",
    "    return doc\n",
    "    \n",
    "# Removemos stop words unicamente en ingles\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "# Lematizacion mediante spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "def lemmatize(doc):\n",
    "    docs = list(nlp.pipe([doc]))\n",
    "    lemas = ' '.join([token.lemma_ for doc in docs for token in doc])\n",
    "    return lemas\n",
    "\n",
    "def leer_datos(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf-8') as archivo: \n",
    "        reader = csv.reader(archivo)\n",
    "        header = next(reader, None)  # Obtener la primera línea como encabezado\n",
    "        for linea in reader:\n",
    "            data.append(linea)\n",
    "\n",
    "    return header, data\n",
    "\n",
    "def put_label(stars):\n",
    "  if stars<=2:\n",
    "    return 0\n",
    "  elif stars==3:\n",
    "    return 1\n",
    "  else:\n",
    "    return 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones de vectorización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vectorizar_tfidf(text):\n",
    "    # TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(text)\n",
    "    \n",
    "    # palabras únicas\n",
    "    feature_names_tfidf = vectorizer.get_feature_names_out()\n",
    "\n",
    "    print(\"Tamaño de Palabras únicas:\", len(feature_names_tfidf))\n",
    "    return tfidf_matrix\n",
    "\n",
    "\n",
    "def tf_vectorizar_tfidf(texts, batch_size=1000):\n",
    "    def process_batch(text_batch):\n",
    "        text_tensor = tf.constant(text_batch)\n",
    "\n",
    "        # Tokenizador básico\n",
    "        tokenizer = tf_text.WhitespaceTokenizer()\n",
    "        tokens = tokenizer.tokenize(text_tensor)\n",
    "\n",
    "        # Crear un vocabulario\n",
    "        unique_tokens, _ = tf.unique(tokens.flat_values)\n",
    "\n",
    "        # Función para calcular frecuencia de documentos (DF)\n",
    "        def doc_freq(term, tokens):\n",
    "            return tf.reduce_sum(tf.cast(tf.reduce_any(tf.equal(tokens, term), axis=-1), tf.float32))\n",
    "\n",
    "        # Calcular DF para cada término\n",
    "        df = tf.map_fn(lambda term: doc_freq(term, tokens), unique_tokens, dtype=tf.float32)\n",
    "\n",
    "        # Calcular IDF\n",
    "        doc_count = tf.cast(len(text_batch), tf.float32)\n",
    "        idf = tf.math.log(doc_count / (df + 1))\n",
    "\n",
    "        # Función para calcular TF (frecuencia de término)\n",
    "        def term_freq(term, tokens):\n",
    "            return tf.reduce_sum(tf.cast(tf.equal(tokens, term), tf.float32), axis=-1)\n",
    "\n",
    "        # Calcular TF-IDF\n",
    "        def compute_tfidf(term):\n",
    "            tf_value = term_freq(term, tokens)\n",
    "            idf_value = idf[tf.where(tf.equal(unique_tokens, term))[0][0]]\n",
    "            return tf_value * idf_value\n",
    "\n",
    "        tfidf_values = tf.map_fn(lambda term: compute_tfidf(term), unique_tokens, dtype=tf.float32)\n",
    "\n",
    "        # Crear un tensor TF-IDF por documento\n",
    "        tfidf_per_doc = tf.stack([tfidf_values[:, i] for i in range(len(text_batch))], axis=1)\n",
    "        return tfidf_per_doc\n",
    "    num_texts = len(texts)\n",
    "    tfidf_results = []\n",
    "    for i in range(0, num_texts, batch_size):\n",
    "        text_batch = texts[i:i + batch_size]\n",
    "        tfidf_batch = process_batch(text_batch)\n",
    "        tfidf_results.append(tfidf_batch)\n",
    "\n",
    "    # Combinar los resultados de los lotes\n",
    "    return tf.concat(tfidf_results, axis=1)\n",
    "\n",
    "def vectorizar_onehot(input):\n",
    "    # One-Hot Encoding\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    onehot_matrix = vectorizer.fit_transform(input)\n",
    "    return onehot_matrix\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    processed_texts = []\n",
    "    doc = list(tqdm(nlp.pipe(texts,disable=['ner','parser']),total=len(texts)))\n",
    "    for text in doc:\n",
    "        \n",
    "        tokens = []\n",
    "        for token in text:\n",
    "            # Lematización\n",
    "            lemma = token.lemma_\n",
    "            # Stemming\n",
    "            stem = stemmer.stem(lemma)\n",
    "            tokens.append(stem)\n",
    "        processed_texts.append(' '.join(tokens))\n",
    "\n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(predict_array):\n",
    "    indice_maximo = predict_array\n",
    "    if indice_maximo==0:\n",
    "        print(\"Sentimiento negativo\")\n",
    "    elif indice_maximo==1:\n",
    "        print(\"Sentimiento neutro\")\n",
    "    elif indice_maximo==2:\n",
    "        print(\"Sentimiento positivo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diccionarios de sentimientos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hiv4_polarity(text,hiv4):\n",
    "    tokens = hiv4.tokenize(text)  \n",
    "    score = hiv4.get_score(tokens)\n",
    "\n",
    "    polarity = score['Polarity']\n",
    "    if polarity > 0.33:\n",
    "        return 2\n",
    "    elif polarity < -0.33:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenizar el texto\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Obtener palabras positivas y negativas del Opinion Lexicon\n",
    "    pos_words = set(opinion_lexicon.positive())\n",
    "    neg_words = set(opinion_lexicon.negative())\n",
    "\n",
    "    # Calcular el sentimiento\n",
    "    pos_score = sum(word in pos_words for word in tokens)\n",
    "    neg_score = sum(word in neg_words for word in tokens)\n",
    "\n",
    "    # Determinar la orientación general\n",
    "    if pos_score > neg_score:\n",
    "        return 1\n",
    "    elif pos_score < neg_score:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adquisición de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "header,raw_dataset=leer_datos(\"/mnt/d/Descargas/Reviews/Reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis exploratorio de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hay 568454 datos. Cada uno con 10 caracteristicas\n",
      "columnas:\n",
      "['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'] \n",
      "\n",
      "['2', 'B00813GRG4', 'A1D87F6ZCVE5NK', 'dll pa', '0', '0', '1', '1346976000', 'Not as Advertised', 'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".'] \n",
      "\n",
      "['3', 'B000LQOCH0', 'ABXLMWJIXXAIN', 'Natalia Corres \"Natalia Corres\"', '1', '1', '4', '1219017600', '\"Delight\" says it all', 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.']\n"
     ]
    }
   ],
   "source": [
    "print(\"hay\",len(raw_dataset),\"datos. Cada uno con\",len(header),\"caracteristicas\")\n",
    "print(\"columnas:\")\n",
    "print(header,\"\\n\")\n",
    "print(raw_dataset[1], \"\\n\")\n",
    "print(raw_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '\"Delight\" says it all', 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.']\n"
     ]
    }
   ],
   "source": [
    "columnas = [6,8,9]\n",
    "reviews = [[row[i] for i in columnas] for row in raw_dataset]\n",
    "print(reviews[2])#Segmentacion, guardamos score, summary y text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82037, 42640, 443777]\n"
     ]
    }
   ],
   "source": [
    "#Contamos cuantos vectores hay por clase\n",
    "#Colocamos el label correcto\n",
    "label_counter = [0,0,0]\n",
    "for i in range(len(reviews)):\n",
    "    if reviews[i][0] == '1' or reviews[i][0] == '2':#Mala opinion\n",
    "        reviews[i][0] = 0\n",
    "        label_counter[0] += 1\n",
    "    elif reviews[i][0] == '3':#Opinion regular\n",
    "        reviews[i][0] = 1\n",
    "        label_counter[1] += 1\n",
    "    else:\n",
    "        reviews[i][0] = 2#Opinion buena\n",
    "        label_counter[2] += 1\n",
    "print(label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 'Addicting Combination of Flavors', \"The combination of dark chocolate, nuts, & sea salt make for an addicting combination. I've already made it through two bars since it arrived in the mail earlier today. I also love the fact that it is made with quality ingredients, so even though it approaches dessert like status, it is more healthily than a candy bar and tastes better than many granola bars, as much as I love granola bars. The salt, chocolate and texture reminds me of Ben & Jerry's Chubby Hubby minus the ice cream of course,\"]\n"
     ]
    }
   ],
   "source": [
    "def randomize(lista):\n",
    "    randomized = lista[:]  \n",
    "    random.shuffle(randomized)\n",
    "    return randomized\n",
    "reviews = randomize(reviews)\n",
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42640, 42640, 42640]\n",
      "127920\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Balance de clases\n",
    "reviews_compressed = []\n",
    "label_check = [0,0,0]\n",
    "for i in range(len(reviews)):\n",
    "    if label_check[0] == label_check[1] == label_check[2] == 42640:\n",
    "        break\n",
    "    if reviews[i][0] == 0 and label_check[0] < 42640:\n",
    "        reviews_compressed.append(reviews[i])\n",
    "        label_check[0] += 1\n",
    "    elif reviews[i][0] == 1 and label_check[1] < 42640:\n",
    "        reviews_compressed.append(reviews[i])\n",
    "        label_check[1] += 1\n",
    "    elif reviews[i][0] == 2 and label_check[2] < 42640:\n",
    "        reviews_compressed.append(reviews[i])\n",
    "        label_check[2] += 1\n",
    "print(label_check)\n",
    "print(len(reviews_compressed))\n",
    "reviews = reviews_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addicting Combination of Flavors\n",
      "Sentimiento positivo\n"
     ]
    }
   ],
   "source": [
    "#Prescindimos de todo menos el score y el summary\n",
    "X=[str(text[1]) for text in reviews]\n",
    "y=[label[0] for label in reviews]\n",
    "print(X[0])\n",
    "prediction(y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Procesamiento del texto\n",
    "nlp=spacy.load('en_core_web_trf')\n",
    "X=[text.lower() for text in X]\n",
    "X=[eliminar_etiquetas(text) for text in X]\n",
    "X=[remover_apostrofes(text) for text in X]\n",
    "X=[remover_especiales(text) for text in X]\n",
    "X=[dobles_espacios(text) for text in X]\n",
    "\n",
    "X_wS=deepcopy(X)\n",
    "\n",
    "X=[remove_stopwords(text) for text in X]\n",
    "X_WoS=deepcopy(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addicting combination flavors Sentimiento positivo\n",
      "gets dog eat Sentimiento positivo\n",
      "product china dont buy Sentimiento negativo\n",
      "bad Sentimiento neutro\n",
      "best idea ever Sentimiento positivo\n",
      "burst refreshment Sentimiento positivo\n",
      "great stuff Sentimiento positivo\n",
      "premium dog food moderate price Sentimiento positivo\n",
      "great cereal Sentimiento positivo\n",
      "excellent brew Sentimiento positivo\n",
      "great coconut oil Sentimiento positivo\n",
      "awesome Sentimiento positivo\n",
      "roasted salty Sentimiento positivo\n",
      "great smoothies kids love Sentimiento positivo\n",
      "numi chocolate tea bags Sentimiento negativo\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(X_WoS[i],end=' ')\n",
    "    prediction(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127920/127920 [11:05<00:00, 192.33it/s]\n"
     ]
    }
   ],
   "source": [
    "#Tokenizacion de reseñas\n",
    "X_wS=preprocess_texts(X_wS)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127920/127920 [08:38<00:00, 246.93it/s]\n"
     ]
    }
   ],
   "source": [
    "X_WoS=preprocess_texts(X_WoS)#Texto sin stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127920\n",
      "great coconut oil\n",
      "Tamaño de Palabras únicas: 12569\n"
     ]
    }
   ],
   "source": [
    "print(len(X_WoS))\n",
    "print(X_WoS[10])\n",
    "tfids_w=vectorizar_tfidf(X_WoS)#Vectorización a texto sin stopwords\n",
    "one_hot_w=vectorizar_onehot(X_WoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10630)\t0.33837600037899407\n",
      "  (0, 2401)\t0.4079830380935052\n",
      "  (0, 4016)\t0.5090840035370278\n",
      "  (0, 7545)\t0.39789795831926666\n",
      "  (0, 3320)\t0.5491468065131392\n"
     ]
    }
   ],
   "source": [
    "print(tfids_w[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análsis de sentimientos con diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n"
     ]
    }
   ],
   "source": [
    "text_analysis = []\n",
    "hiv4 = ps.HIV4()\n",
    "\n",
    "for i in range(len(X_WoS)):\n",
    "    if i % 12000 == 0:\n",
    "        print(\"Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\")\n",
    "    harvard_pol_text = hiv4_polarity(X_WoS[i],hiv4)\n",
    "    nltk_pol_text = analyze_sentiment(X_WoS[i])\n",
    "    text_analysis.append([harvard_pol_text,nltk_pol_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0, 2], [2, 2], [0, 0], [1, 1], [1, 1], [2, 1], [2, 2], [2, 1], [2, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(text_analysis[:10])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict = [0,0]\n",
    "for i in range(len(text_analysis)):\n",
    "    if y[i] == text_analysis[i][0]:\n",
    "        accuracy_dict[0] += 1\n",
    "    if y[i] == text_analysis[i][1]:\n",
    "        accuracy_dict[1] += 1\n",
    "        \n",
    "accuracy_dict[0] = accuracy_dict[0] / 127920\n",
    "accuracy_dict[1] = accuracy_dict[1] / 127920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision usando diccionarios\n",
      "HIV4 ------------------ Opinion Lexicon\n",
      "[0.4104596622889306, 0.33610068792995623]\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision usando diccionarios\")\n",
    "print(\"HIV4 ------------------ Opinion Lexicon\")\n",
    "print(accuracy_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de sentimientos con Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de Palabras únicas: 16889\n"
     ]
    }
   ],
   "source": [
    "tfidf_array=vectorizar_tfidf(X_WoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_reg = LogisticRegression(max_iter=1000)\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold: [0.4793621  0.48397436 0.48217636 0.48182458 0.48542058]\n",
      "Mean accuracy: 0.4825515947467167\n"
     ]
    }
   ],
   "source": [
    "#Regresion logistica\n",
    "scores1 = cross_val_score(logistic_reg, tfidf_array, y, cv=cv)\n",
    "\n",
    "print(\"Accuracy for each fold:\", scores1)\n",
    "print(\"Mean accuracy:\", scores1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold: [0.47005941 0.47170106 0.47088024 0.47416354 0.47494528]\n",
      "Mean accuracy: 0.4723499061913696\n"
     ]
    }
   ],
   "source": [
    "scores2 = cross_val_score(decision_tree, tfidf_array, y, cv=cv)\n",
    "\n",
    "print(\"Accuracy for each fold:\", scores2)\n",
    "print(\"Mean accuracy:\", scores2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold: [0.72568793 0.72291276 0.72424171 0.72267824 0.72127111]\n",
      "Mean accuracy: 0.723358348968105\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svc = SVC()\n",
    "\n",
    "# Definir los hiperparámetros para la búsqueda en cuadrícula\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001]}\n",
    "\n",
    "# Configurar GridSearchCV con n_jobs=-1 para usar todos los núcleos disponibles\n",
    "grid_search = GridSearchCV(svc, param_grid, n_jobs=16, cv=5)\n",
    "scores = cross_val_score(svc, tfidf_array, y, cv=cv,n_jobs=-1)\n",
    "\n",
    "# Imprimir los resultados de la cross-validation\n",
    "print(\"Accuracy for each fold:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de sentimientos usando word embbedings y redes neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "label_encoder=LabelEncoder()\n",
    "label_dict1=label_encoder.fit_transform(y)\n",
    "encoded_y=np.array(y)\n",
    "encoded_y = tf.keras.utils.to_categorical(encoded_y, 3)\n",
    "print(encoded_y[123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capa con embeddings preconstruidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 vectores almacenados\n"
     ]
    }
   ],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"/mnt/d/Repositorios/Tecnologias_de_lenguaje_natural/P2/Practica4/glove.6B/glove.6B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "print(f\"{len(embeddings_dict)} vectores almacenados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(X_WoS)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "encoded_docs = t.texts_to_sequences(X_WoS)\n",
    "\n",
    "# Definimos el maximo size de cada documento\n",
    "max_length = 150\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    " embedding_vector = embeddings_dict.get(word)\n",
    " if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K-Fold] Iteracion:  1\n",
      "0.5978749990463257\n",
      "[K-Fold] Iteracion:  2\n",
      "0.5911666750907898\n",
      "[K-Fold] Iteracion:  3\n",
      "0.5985000133514404\n",
      "[K-Fold] Iteracion:  4\n",
      "0.5980833172798157\n",
      "[K-Fold] Iteracion:  5\n",
      "0.5957083106040955\n",
      "Precision promedio: 59.63%\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "acc_scores = []\n",
    "\n",
    "iteracion = 0\n",
    "# Iteracion del modelo por KFolds\n",
    "for train_index, test_index in kf.split(padded_docs, y):\n",
    "    iteracion += 1\n",
    "    print(\"[K-Fold] Iteracion: \", iteracion)\n",
    "\n",
    "    # Definición del modelo\n",
    "    model = tf.keras.models.Sequential()\n",
    "    e = tf.keras.layers.Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)\n",
    "    model.add(e)\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Dividir los datos en entrenamiento y prueba\n",
    "    X_train, X_test = padded_docs[train_index], padded_docs[test_index]\n",
    "    y_train, y_test = encoded_y[train_index], encoded_y[test_index]\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # Entrenamiento del modelo\n",
    "    model.fit(X_train, y_train, epochs=10, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "    # Evaluación del modelo\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    acc_scores.append(accuracy)\n",
    "    print(f\"Iteracion {iteracion} - Precisión: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Precisión promedio: {:.2f}%\".format(np.mean(acc_scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capa con embeddings aprendidos del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "# integer encode the documents\n",
    "encoded_docs = [vectorizar_onehot(d, vocab_size) for d in X_WoS]\n",
    "#print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 100\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K-Fold] Iteracion:  1\n",
      "0.6744583249092102\n",
      "[K-Fold] Iteracion:  2\n",
      "0.6714583039283752\n",
      "[K-Fold] Iteracion:  3\n",
      "0.6805833578109741\n",
      "[K-Fold] Iteracion:  4\n",
      "0.6728749871253967\n",
      "[K-Fold] Iteracion:  5\n",
      "0.6694999933242798\n",
      "Precisión promedio: 67.38%\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "acc_scores = []\n",
    "\n",
    "iteracion = 0\n",
    "\n",
    "# Iteración del modelo por KFolds\n",
    "for train_index, test_index in kf.split(padded_docs, y):\n",
    "    iteracion += 1\n",
    "    print(\"[K-Fold] Iteracion: \", iteracion)\n",
    "\n",
    "    model = tf.keras.layers.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size, 8, input_length=max_length))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    X_train, X_test = padded_docs[train_index], padded_docs[test_index]\n",
    "    y_train, y_test = encoded_y[train_index], encoded_y[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc_scores.append(accuracy)\n",
    "    print(accuracy)\n",
    "\n",
    "print(\"Precisión promedio: {:.2f}%\".format(np.mean(acc_scores) * 100)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
