{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cabecera\n",
    "Montero Barraza Álvaro David\n",
    "\n",
    "5BV1\n",
    "\n",
    "Ingenieria en Inteligencia Artificial \n",
    "\n",
    "\n",
    "Práctica 5. Analisis de sentimientos\n",
    "\n",
    "A lo largo de esta practica, utilizaremos diversos modelos para el analisis de sentimientos, utilizando modelos de machine learning, diccionarios y redes neuronales. Ademas de que realizaremos un analisis de una coleccion de reseñas de comida fina en amazon y finalmente compararemos los distintos modelos al ponerlos a prueba con el dataset normalizado y vectorizado segun el modelo que corresponda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvaromontero/.local/lib/python3.10/site-packages/cupy/_environment.py:447: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy-cuda113, cupy-cuda12x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n",
      "/home/alvaromontero/.local/lib/python3.10/site-packages/cupy/_environment.py:447: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy-cuda113, cupy-cuda12x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n",
      "/home/alvaromontero/.local/lib/python3.10/site-packages/cupy/_environment.py:447: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy-cuda113, cupy-cuda12x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n",
      "/home/alvaromontero/.local/lib/python3.10/site-packages/cupy/_environment.py:447: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy-cuda113, cupy-cuda12x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n",
      "2024-06-24 02:11:07.102117: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-24 02:11:07.299857: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-24 02:11:07.299889: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-24 02:11:07.331034: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-24 02:11:07.395464: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-24 02:11:08.096726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /home/alvaromontero/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/alvaromontero/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "import pysentiment2 as ps\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Funciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvaromontero/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/alvaromontero/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/alvaromontero/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "def randomize(lista):\n",
    "    randomized = lista[:]  \n",
    "    random.shuffle(randomized)\n",
    "    return randomized\n",
    "\n",
    "# Elimina etiquetas HTML y comentarios dentro del propio texto como indicaciones extra entre parentesis o comentarios entre --\n",
    "def eliminar_etiquetas(text):\n",
    "    pattern = re.compile(r'<[^>]+>|\\([^)]+\\)|--[^\\-]+--')\n",
    "    doc = pattern.sub('', text)\n",
    "    return doc\n",
    "\n",
    "# Removemos apostrofes para quedarnos con palabras como cant couldnt o dont\n",
    "def remover_apostrofes(text):\n",
    "    pattern = r\"\\\\?'\"\n",
    "    doc = re.sub(pattern, '', text)\n",
    "    return doc\n",
    "# Nos quedamos con solo alfabeto eliminando caracteres especiales    \n",
    "def remover_especiales(text):\n",
    "    pattern = r\"[^a-zA-Z\\s.,]\"\n",
    "    doc = re.sub(pattern, ' ', text)\n",
    "    return doc\n",
    "# Quitamos dobles espacios para tener texto espaciado solamente por un espacio por palabra\n",
    "def dobles_espacios(text):\n",
    "    pattern = r'\\s+'\n",
    "    doc = re.sub(pattern, ' ', text)\n",
    "    return doc\n",
    "    \n",
    "# Removemos stop words unicamente en ingles\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "# Lematizacion mediante spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "def lemmatize(doc):\n",
    "    docs = list(nlp.pipe([doc]))\n",
    "    lemas = ' '.join([token.lemma_ for doc in docs for token in doc])\n",
    "    return lemas\n",
    "\n",
    "def leer_datos(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf-8') as archivo: \n",
    "        reader = csv.reader(archivo)\n",
    "        header = next(reader, None)  # Obtener la primera línea como encabezado\n",
    "        for linea in reader:\n",
    "            data.append(linea)\n",
    "\n",
    "    return header, data\n",
    "\n",
    "def put_label(stars):\n",
    "  if stars<=2:\n",
    "    return 0\n",
    "  elif stars==3:\n",
    "    return 1\n",
    "  else:\n",
    "    return 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones de vectorización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "def vectorizar_tfidf(text):\n",
    "    # TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(text)\n",
    "    \n",
    "    # palabras únicas\n",
    "    feature_names_tfidf = vectorizer.get_feature_names_out()\n",
    "\n",
    "    print(\"Tamaño de Palabras únicas:\", len(feature_names_tfidf))\n",
    "    return tfidf_matrix\n",
    "\n",
    "def vectorizar_onehot(input):\n",
    "    # One-Hot Encoding\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    onehot_matrix = vectorizer.fit_transform(input)\n",
    "    return onehot_matrix\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    processed_texts = []\n",
    "    doc = list(tqdm(nlp.pipe(texts,disable=['ner','parser']),total=len(texts)))\n",
    "    for text in doc:\n",
    "        \n",
    "        tokens = []\n",
    "        for token in text:\n",
    "            # Lematización\n",
    "            lemma = token.lemma_\n",
    "            # Stemming\n",
    "            stem = stemmer.stem(lemma)\n",
    "            tokens.append(stem)\n",
    "        processed_texts.append(' '.join(tokens))\n",
    "\n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diccionarios de sentimientos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hiv4_polarity(text,hiv4):\n",
    "    tokens = hiv4.tokenize(text)  \n",
    "    score = hiv4.get_score(tokens)\n",
    "\n",
    "    polarity = score['Polarity']\n",
    "    if polarity > 0.33:\n",
    "        return 2\n",
    "    elif polarity < -0.33:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenizar el texto\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Obtener palabras positivas y negativas del Opinion Lexicon\n",
    "    pos_words = set(opinion_lexicon.positive())\n",
    "    neg_words = set(opinion_lexicon.negative())\n",
    "\n",
    "    # Calcular el sentimiento\n",
    "    pos_score = sum(word in pos_words for word in tokens)\n",
    "    neg_score = sum(word in neg_words for word in tokens)\n",
    "\n",
    "    # Determinar la orientación general\n",
    "    if pos_score > neg_score:\n",
    "        return 1\n",
    "    elif pos_score < neg_score:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adquisición de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "header,raw_dataset=leer_datos(\"/mnt/d/Descargas/Reviews/Reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis exploratorio de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hay 568454 datos. Cada uno con 10 caracteristicas\n",
      "columnas:\n",
      "['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'] \n",
      "\n",
      "['2', 'B00813GRG4', 'A1D87F6ZCVE5NK', 'dll pa', '0', '0', '1', '1346976000', 'Not as Advertised', 'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".'] \n",
      "\n",
      "['3', 'B000LQOCH0', 'ABXLMWJIXXAIN', 'Natalia Corres \"Natalia Corres\"', '1', '1', '4', '1219017600', '\"Delight\" says it all', 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.']\n"
     ]
    }
   ],
   "source": [
    "print(\"hay\",len(raw_dataset),\"datos. Cada uno con\",len(header),\"caracteristicas\")\n",
    "print(\"columnas:\")\n",
    "print(header,\"\\n\")\n",
    "print(raw_dataset[1], \"\\n\")\n",
    "print(raw_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '\"Delight\" says it all', 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.']\n"
     ]
    }
   ],
   "source": [
    "columnas = [6,8,9]\n",
    "reviews = [[row[i] for i in columnas] for row in raw_dataset]\n",
    "print(reviews[2])#Segmentacion, guardamos score, summary y text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82037, 42640, 443777]\n"
     ]
    }
   ],
   "source": [
    "#Contamos cuantos vectores hay por clase\n",
    "#Colocamos el label correcto\n",
    "label_counter = [0,0,0]\n",
    "for i in range(len(reviews)):\n",
    "    if reviews[i][0] == '1' or reviews[i][0] == '2':#Mala opinion\n",
    "        reviews[i][0] = 0\n",
    "        label_counter[0] += 1\n",
    "    elif reviews[i][0] == '3':#Opinion regular\n",
    "        reviews[i][0] = 1\n",
    "        label_counter[1] += 1\n",
    "    else:\n",
    "        reviews[i][0] = 2#Opinion buena\n",
    "        label_counter[2] += 1\n",
    "print(label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 'Delicious tea with taste of Matcha', \"My husband and I love the Matcha green tea shots at Jamba juice as well as regular green tea. When I saw this tea in the local grocery store I thought I'd give it a try.<br /><br />It isn't as strong as I hoped it might be, but definitely has the Match flavor.<br /><br />It's a wonderful tea to drink at any time of the day. Light in flavor...can't really accidentally oversteep to bitterness the way you can with other greens).<br /><br />Overall, I'd recommend this tea, though it could be a tad less expensive.\"]\n"
     ]
    }
   ],
   "source": [
    "def randomize(lista):\n",
    "    randomized = lista[:]  \n",
    "    random.shuffle(randomized)\n",
    "    return randomized\n",
    "reviews = randomize(reviews)\n",
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42640, 42640, 42640]\n",
      "127920\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Balance de clases\n",
    "reviews_compressed = []\n",
    "label_check = [0,0,0]\n",
    "for i in range(len(reviews)):\n",
    "    if label_check[0] == label_check[1] == label_check[2] == 42640:\n",
    "        break\n",
    "    if reviews[i][0] == 0 and label_check[0] < 42640:\n",
    "        reviews_compressed.append(reviews[i])\n",
    "        label_check[0] += 1\n",
    "    elif reviews[i][0] == 1 and label_check[1] < 42640:\n",
    "        reviews_compressed.append(reviews[i])\n",
    "        label_check[1] += 1\n",
    "    elif reviews[i][0] == 2 and label_check[2] < 42640:\n",
    "        reviews_compressed.append(reviews[i])\n",
    "        label_check[2] += 1\n",
    "print(label_check)\n",
    "print(len(reviews_compressed))\n",
    "reviews = reviews_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delicious tea with taste of Matcha\n"
     ]
    }
   ],
   "source": [
    "#Prescindimos de todo menos el score y el summary\n",
    "X=[str(text[1]) for text in reviews]\n",
    "y=[label[0] for label in reviews]\n",
    "print(X[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Procesamiento del texto\n",
    "nlp=spacy.load('en_core_web_trf')\n",
    "X=[text.lower() for text in X]\n",
    "X=[eliminar_etiquetas(text) for text in X]\n",
    "X=[remover_apostrofes(text) for text in X]\n",
    "X=[remover_especiales(text) for text in X]\n",
    "X=[dobles_espacios(text) for text in X]\n",
    "\n",
    "X_wS=deepcopy(X)\n",
    "\n",
    "X=[remove_stopwords(text) for text in X]\n",
    "X_WoS=deepcopy(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127920/127920 [10:38<00:00, 200.50it/s]\n"
     ]
    }
   ],
   "source": [
    "#Tokenizacion de reseñas\n",
    "X_wS=preprocess_texts(X_wS)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127920/127920 [08:19<00:00, 256.24it/s]\n"
     ]
    }
   ],
   "source": [
    "X_WoS=preprocess_texts(X_WoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127920\n",
      "great taste .\n",
      "Tamaño de Palabras únicas: 16800\n"
     ]
    }
   ],
   "source": [
    "print(len(X_WoS))\n",
    "print(X_WoS[10])\n",
    "tfids_w=vectorizar_tfidf(X_WoS)#Vectorización a texto sin stopwords\n",
    "one_hot_w=vectorizar_onehot(X_WoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3779)\t0.44014662237877905\n",
      "  (0, 14665)\t0.3955396695250961\n",
      "  (0, 14610)\t0.36046950736387695\n",
      "  (0, 8970)\t0.7210277767890597\n"
     ]
    }
   ],
   "source": [
    "print(tfids_w[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análsis de sentimientos con diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n",
      "Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\n"
     ]
    }
   ],
   "source": [
    "text_analysis = []\n",
    "hiv4 = ps.HIV4()\n",
    "\n",
    "for i in range(len(X_WoS)):\n",
    "    if i % 12000 == 0:\n",
    "        print(\"Bloque aun siendo leido (de un total de 10 bloques de los 120k datos)\")\n",
    "    harvard_pol_text = hiv4_polarity(X_WoS[i],hiv4)\n",
    "    nltk_pol_text = analyze_sentiment(X_WoS[i])\n",
    "    text_analysis.append([harvard_pol_text,nltk_pol_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1], [2, 2], [2, 2], [0, 2], [2, 2], [2, 2], [2, 1], [0, 2], [0, 1], [2, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(text_analysis[:10])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict = [0,0]\n",
    "for i in range(len(text_analysis)):\n",
    "    if y[i] == text_analysis[i][0]:\n",
    "        accuracy_dict[0] += 1\n",
    "    if y[i] == text_analysis[i][1]:\n",
    "        accuracy_dict[1] += 1\n",
    "        \n",
    "accuracy_dict[0] = accuracy_dict[0] / 120000\n",
    "accuracy_dict[1] = accuracy_dict[1] / 120000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presicion usando diccionarios\n",
      "HIV4 ------------------ Opinion Lexicon\n",
      "[0.43783333333333335, 0.35923333333333335]\n"
     ]
    }
   ],
   "source": [
    "print(\"Presicion usando diccionarios\")\n",
    "print(\"HIV4 ------------------ Opinion Lexicon\")\n",
    "print(accuracy_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de sentimientos con Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder=LabelEncoder()\n",
    "label_dict1=label_encoder.fit_transform(y)\n",
    "y=np.array(y)\n",
    "y = tf.keras.utils.to_categorical(y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(y[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 02:15:51.465104: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: required broadcastable shapes\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m tfidf_tensor\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m24\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     tfidf_tensor\u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_array\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43maux\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mtfidf_tensor\u001b[49m\n\u001b[1;32m      8\u001b[0m     aux\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mbatch_size\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2] name: "
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf_array = tfids_w.toarray()\n",
    "print(tfidf_array.shape[0])\n",
    "batch_size=5330\n",
    "aux=5330\n",
    "tfidf_tensor=[]\n",
    "for i in range(24):\n",
    "    tfidf_tensor= tf.convert_to_tensor(tfidf_array[i*batch_size:aux-1], dtype=tf.float32)+tfidf_tensor\n",
    "    aux+=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5329\n"
     ]
    }
   ],
   "source": [
    "print(len(tfidf_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mtfidf_tensor\u001b[49m,y,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_tensor,y,random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7390)\t0.6719517248376479\n",
      "  (0, 10132)\t0.7405949496774266[0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(str(X_train[32]) + str(y_train[32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 12466)\n"
     ]
    }
   ],
   "source": [
    "#Modelo de regresion logistica\n",
    "print(tfids_w[1].shape)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 01:32:29.221576: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at serialize_sparse_op.cc:388 : INVALID_ARGUMENT: indices[4] = [3,4877] is out of order. Many sparse ops require sorted indices.\n",
      "    Use `tf.sparse.reorder` to create a correctly ordered copy.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__SerializeManySparse_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[4] = [3,4877] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[1;32m      2\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Monitorear la pérdida de validación\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,         \u001b[38;5;66;03m# Número de épocas sin mejora antes de detener el entrenamiento\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Restaurar los mejores pesos al finalizar el entrenamiento\u001b[39;00m\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__SerializeManySparse_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[4] = [3,4877] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse] name: "
     ]
    }
   ],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitorear la pérdida de validación\n",
    "    patience=10,         # Número de épocas sin mejora antes de detener el entrenamiento\n",
    "    restore_best_weights=True  # Restaurar los mejores pesos al finalizar el entrenamiento\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=1000, batch_size=32,callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
